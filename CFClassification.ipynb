{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "electoral-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from parse import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "failing-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show classification report for Cross Validation\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "covered-airplane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################\n",
      "Number of Rows of Dataframe:\n",
      "1123\n",
      "Number of Columns of Dataframe:\n",
      "59\n",
      "\n",
      "####################################################################\n",
      "Threshold for number of NULLs in a column: 0.1095\n",
      "Number of Columns before Parsing for Too Many NULLs in a column:\n",
      "59\n",
      "Number of Columns after Parsing for Too Many NULLs in a column:\n",
      "51\n",
      "\n",
      "Columns Removed:\n",
      "B1_b5\n",
      "B4_a1\n",
      "B4_a3\n",
      "B4_a4\n",
      "B4_a6\n",
      "B4_b1\n",
      "B4_b3\n",
      "B5_a1\n",
      "\n",
      "####################################################################\n",
      "Number of Rows before Parsing NULLs in data:\n",
      "1123\n",
      "Number of Rows after Parsing NULLs in data:\n",
      "1007\n",
      "\n",
      "####################################################################\n",
      "Number of Columns after dropping A1_2, B1_b4, B2_c3, B4_b2 for inconsistent data types:\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Pre-parse the dataset\n",
    "data = preprocess(\"rawfile_blood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "prime-listening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################\n",
      "Labels with frequencies:\n",
      "Frail: 7\n",
      "Frail + MCI: 76\n",
      "MCI: 133\n",
      "Prefrail + MCI: 231\n",
      "Prefrail: 221\n",
      "Robust: 339\n"
     ]
    }
   ],
   "source": [
    "# Initialise counters for each condition\n",
    "frail = 0\n",
    "frail_mci = 0\n",
    "mci = 0\n",
    "prefrail_mci = 0\n",
    "prefrail = 0\n",
    "robust = 0\n",
    "\n",
    "# Count rows of data for each condition\n",
    "for i in range(0, len(data)):\n",
    "\tif data.at[i, 'condition'] == 'frail':\n",
    "\t\tfrail += 1\n",
    "\telif data.at[i, 'condition'] == 'frail_mci':\n",
    "\t\tfrail_mci += 1\n",
    "\telif data.at[i, 'condition'] == 'mci':\n",
    "\t\tmci += 1\n",
    "\telif data.at[i, 'condition'] == 'prefrail_mci':\n",
    "\t\tprefrail_mci += 1\n",
    "\telif data.at[i, 'condition'] == 'prefrail':\n",
    "\t\tprefrail += 1\n",
    "\telif data.at[i, 'condition'] == 'robust':\n",
    "\t\trobust += 1\n",
    "        \n",
    "# Display number of rows (frequency) for each condition (label)\n",
    "print(\"\\n####################################################################\")\n",
    "print(\"Labels with frequencies:\")\n",
    "print(\"Frail:\", frail)\n",
    "print(\"Frail + MCI:\", frail_mci)\n",
    "print(\"MCI:\", mci)\n",
    "print(\"Prefrail + MCI:\", prefrail_mci)\n",
    "print(\"Prefrail:\", prefrail)\n",
    "print(\"Robust:\", robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "perceived-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtag</th>\n",
       "      <th>condition</th>\n",
       "      <th>A1_1</th>\n",
       "      <th>A2_1</th>\n",
       "      <th>A3_1</th>\n",
       "      <th>B1_a</th>\n",
       "      <th>B1_a1</th>\n",
       "      <th>B1_a2</th>\n",
       "      <th>B1_a3</th>\n",
       "      <th>B1_a4</th>\n",
       "      <th>...</th>\n",
       "      <th>B2_d6</th>\n",
       "      <th>B2_d7</th>\n",
       "      <th>B2_d8</th>\n",
       "      <th>B2_d9</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4_a2</th>\n",
       "      <th>B4_a5</th>\n",
       "      <th>B5_a2</th>\n",
       "      <th>B5_a3</th>\n",
       "      <th>B6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ME02646</td>\n",
       "      <td>frail</td>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>46.5</td>\n",
       "      <td>121</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.37</td>\n",
       "      <td>95</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ME03109</td>\n",
       "      <td>frail</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>55.6</td>\n",
       "      <td>142</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>87</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ME06997</td>\n",
       "      <td>frail</td>\n",
       "      <td>441</td>\n",
       "      <td>20</td>\n",
       "      <td>76.8</td>\n",
       "      <td>105</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>2.14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ME07149</td>\n",
       "      <td>frail</td>\n",
       "      <td>265</td>\n",
       "      <td>16</td>\n",
       "      <td>47.2</td>\n",
       "      <td>122</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>86</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.06</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ME07700</td>\n",
       "      <td>frail</td>\n",
       "      <td>425</td>\n",
       "      <td>14</td>\n",
       "      <td>31.3</td>\n",
       "      <td>124</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>85</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mtag condition  A1_1  A2_1  A3_1  B1_a  B1_a1  B1_a2  B1_a3  B1_a4  ...  \\\n",
       "0  ME02646     frail   196    24  46.5   121   3.93   0.37     95     31  ...   \n",
       "1  ME03109     frail   200    23  55.6   142   4.82   0.42     87     30  ...   \n",
       "2  ME06997     frail   441    20  76.8   105   4.54   0.41     90     30  ...   \n",
       "3  ME07149     frail   265    16  47.2   122   4.53   0.39     86     27  ...   \n",
       "4  ME07700     frail   425    14  31.3   124   4.44   0.38     85     28  ...   \n",
       "\n",
       "   B2_d6  B2_d7  B2_d8  B2_d9   B3  B4_a2  B4_a5  B5_a2  B5_a3   B6  \n",
       "0      7     12     13      6  0.2    6.0  1.011   1.14    4.1  5.9  \n",
       "1      7     20     17     26  3.1    5.0  1.011   3.25    4.6  8.5  \n",
       "2      5     16     19     15  1.4    7.0  1.023   2.14    4.0  6.4  \n",
       "3      8     24     19     21  2.1    5.5  1.012   1.06    4.7  6.1  \n",
       "4      6     20     23     23  6.0    5.5  1.013   1.95    3.8  5.8  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "appreciated-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mtag', 'condition', 'A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
       "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
       "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
       "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
       "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
       "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "terminal-lindsay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "robust          339\n",
       "prefrail_mci    231\n",
       "prefrail        221\n",
       "mci             133\n",
       "frail_mci        76\n",
       "frail             7\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "mental-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtag</th>\n",
       "      <th>condition</th>\n",
       "      <th>A1_1</th>\n",
       "      <th>A2_1</th>\n",
       "      <th>A3_1</th>\n",
       "      <th>B1_a</th>\n",
       "      <th>B1_a1</th>\n",
       "      <th>B1_a2</th>\n",
       "      <th>B1_a3</th>\n",
       "      <th>B1_a4</th>\n",
       "      <th>...</th>\n",
       "      <th>B2_d6</th>\n",
       "      <th>B2_d7</th>\n",
       "      <th>B2_d8</th>\n",
       "      <th>B2_d9</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4_a2</th>\n",
       "      <th>B4_a5</th>\n",
       "      <th>B5_a2</th>\n",
       "      <th>B5_a3</th>\n",
       "      <th>B6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ME02646</td>\n",
       "      <td>5</td>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>46.5</td>\n",
       "      <td>121</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.37</td>\n",
       "      <td>95</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ME03109</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>55.6</td>\n",
       "      <td>142</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>87</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ME06997</td>\n",
       "      <td>5</td>\n",
       "      <td>441</td>\n",
       "      <td>20</td>\n",
       "      <td>76.8</td>\n",
       "      <td>105</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>2.14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mtag  condition  A1_1  A2_1  A3_1  B1_a  B1_a1  B1_a2  B1_a3  B1_a4  \\\n",
       "0  ME02646          5   196    24  46.5   121   3.93   0.37     95     31   \n",
       "1  ME03109          5   200    23  55.6   142   4.82   0.42     87     30   \n",
       "2  ME06997          5   441    20  76.8   105   4.54   0.41     90     30   \n",
       "\n",
       "   ...  B2_d6  B2_d7  B2_d8  B2_d9   B3  B4_a2  B4_a5  B5_a2  B5_a3   B6  \n",
       "0  ...      7     12     13      6  0.2    6.0  1.011   1.14    4.1  5.9  \n",
       "1  ...      7     20     17     26  3.1    5.0  1.011   3.25    4.6  8.5  \n",
       "2  ...      5     16     19     15  1.4    7.0  1.023   2.14    4.0  6.4  \n",
       "\n",
       "[3 rows x 47 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "religious-gospel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtag</th>\n",
       "      <th>condition</th>\n",
       "      <th>A1_1</th>\n",
       "      <th>A2_1</th>\n",
       "      <th>A3_1</th>\n",
       "      <th>B1_a</th>\n",
       "      <th>B1_a1</th>\n",
       "      <th>B1_a2</th>\n",
       "      <th>B1_a3</th>\n",
       "      <th>B1_a4</th>\n",
       "      <th>...</th>\n",
       "      <th>B2_d6</th>\n",
       "      <th>B2_d7</th>\n",
       "      <th>B2_d8</th>\n",
       "      <th>B2_d9</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4_a2</th>\n",
       "      <th>B4_a5</th>\n",
       "      <th>B5_a2</th>\n",
       "      <th>B5_a3</th>\n",
       "      <th>B6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>MV00454</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>19</td>\n",
       "      <td>67.5</td>\n",
       "      <td>138</td>\n",
       "      <td>4.66</td>\n",
       "      <td>0.42</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.29</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>MV00456</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>18</td>\n",
       "      <td>51.0</td>\n",
       "      <td>139</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.42</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.015</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>MV00460</td>\n",
       "      <td>0</td>\n",
       "      <td>418</td>\n",
       "      <td>17</td>\n",
       "      <td>61.0</td>\n",
       "      <td>122</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.38</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.005</td>\n",
       "      <td>3.58</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>MV00502</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>18</td>\n",
       "      <td>43.1</td>\n",
       "      <td>136</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.43</td>\n",
       "      <td>94</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.92</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>MV00510</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>24</td>\n",
       "      <td>55.9</td>\n",
       "      <td>127</td>\n",
       "      <td>4.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.017</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mtag  condition  A1_1  A2_1  A3_1  B1_a  B1_a1  B1_a2  B1_a3  B1_a4  \\\n",
       "1002  MV00454          0   220    19  67.5   138   4.66   0.42     91     30   \n",
       "1003  MV00456          0   334    18  51.0   139   4.63   0.42     91     30   \n",
       "1004  MV00460          0   418    17  61.0   122   4.18   0.38     90     29   \n",
       "1005  MV00502          0   393    18  43.1   136   4.57   0.43     94     30   \n",
       "1006  MV00510          0   371    24  55.9   127   4.41   0.40     90     29   \n",
       "\n",
       "      ...  B2_d6  B2_d7  B2_d8  B2_d9   B3  B4_a2  B4_a5  B5_a2  B5_a3   B6  \n",
       "1002  ...     20     10     17      8  6.6    7.0  1.015   1.29    4.5  6.2  \n",
       "1003  ...     16     22     35     40  1.0    6.0  1.015   1.88    3.9  5.6  \n",
       "1004  ...     19     20     23     15  0.4    6.5  1.005   3.58    4.0  5.6  \n",
       "1005  ...     13     11     22     23  0.7    7.0  1.009   0.92    4.1  6.0  \n",
       "1006  ...     13     14     16     12  7.5    8.0  1.017   2.45    4.5  6.2  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "appropriate-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mtag', 'condition', 'A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
       "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
       "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
       "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
       "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
       "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "suspected-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "unusual-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "subject-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 339, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7})\n"
     ]
    }
   ],
   "source": [
    "# Summarise the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "attempted-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "X, y = undersample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "existing-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7})\n"
     ]
    }
   ],
   "source": [
    "# Summarise the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "korean-charter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922,)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "stuffed-writing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(922, 45)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "alternate-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "absolute-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 254, 1: 254, 2: 254, 3: 254, 4: 254, 5: 254})\n"
     ]
    }
   ],
   "source": [
    "# Summarise the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "medical-finder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1524,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "traditional-tsunami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1524, 45)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "earlier-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Using the entire dataset as both the train and test sets without splitting into separate train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "purple-resort",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.434\n",
      "Linear Discriminant Analysis: 0.51\n",
      "K-Nearest Neigbors: 0.763\n",
      "Classification and Regression Trees: 1.0\n",
      "Gaussian Naive Bayes: 0.465\n",
      "Support Vector Machines: 1.0\n",
      "Random Forest Classifier: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Logistics Regression\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "print(\"Logistic Regression:\", log_model.score(X, y).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X, y).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X, y).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X, y).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X, y).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X, y).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X, y).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fuzzy-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Splitting the dataset into separate train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "simple-entertainment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.383\n",
      "Linear Discriminant Analysis: 0.464\n",
      "K-Nearest Neigbors: 0.558\n",
      "Classification and Regression Trees: 0.545\n",
      "Gaussian Naive Bayes: 0.459\n",
      "Support Vector Machines: 0.501\n",
      "Random Forest Classifier: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "extraordinary-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.40 accuracy with a standard deviation of 0.03\n",
      "Linear Discriminant Analysis: 0.45 accuracy with a standard deviation of 0.06\n",
      "K-Nearest Neighbors: 0.61 accuracy with a standard deviation of 0.06\n",
      "Classification and Regression Trees: 0.56 accuracy with a standard deviation of 0.05\n",
      "Gaussian Naive Bayes: 0.45 accuracy with a standard deviation of 0.04\n",
      "Support Vector Machines: 0.52 accuracy with a standard deviation of 0.03\n",
      "Random Forest Classifier: 0.75 accuracy with a standard deviation of 0.08\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=10)\n",
    "print(\"Logistic Regression: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=10)\n",
    "print(\"Linear Discriminant Analysis: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10)\n",
    "print(\"K-Nearest Neighbors: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10)\n",
    "print(\"Classification and Regression Trees: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10)\n",
    "print(\"Gaussian Naive Bayes: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10)\n",
    "print(\"Support Vector Machines: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "# Undersample the majority class\n",
    "# Define undersample strategy\n",
    "\n",
    "# 75% of majority class\n",
    "# sampling_strategy = {0: 254, 1: 231, 2: 221, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 50% of majority class\n",
    "# sampling_strategy = {0: 170, 1: 170, 2: 170, 3: 133, 4: 76, 5: 7}\n",
    "\n",
    "# 25% of majority class\n",
    "# sampling_strategy = {0: 85, 1: 85, 2: 85, 3: 85, 4: 76, 5: 7}\n",
    "\n",
    "\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# 50% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0413)\n",
    "\n",
    "# 25% of majority class\n",
    "# undersample = RandomUnderSampler(sampling_strategy=0.0826)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10)\n",
    "print(\"Random Forest Classifier: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "saving-capital",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.44      0.34        34\n",
      "           1       0.44      0.12      0.19        34\n",
      "           2       0.26      0.18      0.21        34\n",
      "           3       0.29      0.26      0.28        34\n",
      "           4       0.52      0.38      0.44        34\n",
      "           5       0.56      1.00      0.72        34\n",
      "\n",
      "    accuracy                           0.40       204\n",
      "   macro avg       0.39      0.40      0.36       204\n",
      "weighted avg       0.39      0.40      0.36       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.26      0.26        34\n",
      "           1       0.17      0.12      0.14        34\n",
      "           2       0.20      0.12      0.15        34\n",
      "           3       0.26      0.29      0.28        34\n",
      "           4       0.33      0.29      0.31        34\n",
      "           5       0.58      1.00      0.73        34\n",
      "\n",
      "    accuracy                           0.35       204\n",
      "   macro avg       0.30      0.35      0.31       204\n",
      "weighted avg       0.30      0.35      0.31       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.21      0.24        34\n",
      "           1       0.20      0.15      0.17        34\n",
      "           2       0.38      0.15      0.21        34\n",
      "           3       0.39      0.35      0.37        34\n",
      "           4       0.30      0.50      0.38        34\n",
      "           5       0.59      0.94      0.73        34\n",
      "\n",
      "    accuracy                           0.38       204\n",
      "   macro avg       0.36      0.38      0.35       204\n",
      "weighted avg       0.36      0.38      0.35       204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.21      0.24        34\n",
      "           1       0.29      0.12      0.17        34\n",
      "           2       0.18      0.18      0.18        34\n",
      "           3       0.14      0.15      0.14        34\n",
      "           4       0.33      0.38      0.35        34\n",
      "           5       0.60      1.00      0.75        34\n",
      "\n",
      "    accuracy                           0.34       204\n",
      "   macro avg       0.30      0.34      0.30       204\n",
      "weighted avg       0.30      0.34      0.30       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.27      0.26        33\n",
      "           1       0.21      0.15      0.17        34\n",
      "           2       0.19      0.09      0.12        34\n",
      "           3       0.38      0.35      0.36        34\n",
      "           4       0.32      0.44      0.37        34\n",
      "           5       0.69      1.00      0.82        34\n",
      "\n",
      "    accuracy                           0.38       203\n",
      "   macro avg       0.34      0.38      0.35       203\n",
      "weighted avg       0.34      0.38      0.35       203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.15      0.17        34\n",
      "           1       0.21      0.18      0.19        34\n",
      "           2       0.36      0.27      0.31        33\n",
      "           3       0.28      0.24      0.25        34\n",
      "           4       0.38      0.35      0.36        34\n",
      "           5       0.54      1.00      0.70        34\n",
      "\n",
      "    accuracy                           0.36       203\n",
      "   macro avg       0.33      0.36      0.33       203\n",
      "weighted avg       0.33      0.36      0.33       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.41      0.35        34\n",
      "           1       0.09      0.06      0.07        33\n",
      "           2       0.27      0.24      0.25        34\n",
      "           3       0.38      0.32      0.35        34\n",
      "           4       0.46      0.35      0.40        34\n",
      "           5       0.69      1.00      0.82        34\n",
      "\n",
      "    accuracy                           0.40       203\n",
      "   macro avg       0.37      0.40      0.37       203\n",
      "weighted avg       0.37      0.40      0.37       203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.35      0.34        34\n",
      "           1       0.16      0.09      0.11        34\n",
      "           2       0.25      0.26      0.26        34\n",
      "           3       0.29      0.21      0.25        33\n",
      "           4       0.47      0.44      0.45        34\n",
      "           5       0.62      1.00      0.76        34\n",
      "\n",
      "    accuracy                           0.39       203\n",
      "   macro avg       0.35      0.39      0.36       203\n",
      "weighted avg       0.35      0.39      0.36       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.15      0.17        34\n",
      "           1       0.25      0.12      0.16        34\n",
      "           2       0.30      0.35      0.32        34\n",
      "           3       0.42      0.32      0.37        34\n",
      "           4       0.37      0.39      0.38        33\n",
      "           5       0.56      1.00      0.72        34\n",
      "\n",
      "    accuracy                           0.39       203\n",
      "   macro avg       0.35      0.39      0.35       203\n",
      "weighted avg       0.35      0.39      0.35       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.53      0.48        34\n",
      "           1       0.27      0.18      0.21        34\n",
      "           2       0.38      0.29      0.33        34\n",
      "           3       0.35      0.26      0.30        34\n",
      "           4       0.46      0.56      0.51        34\n",
      "           5       0.70      1.00      0.82        33\n",
      "\n",
      "    accuracy                           0.47       203\n",
      "   macro avg       0.43      0.47      0.44       203\n",
      "weighted avg       0.43      0.47      0.44       203\n",
      "\n",
      "[0.39705882 0.34803922 0.38235294 0.33823529 0.38423645 0.36453202\n",
      " 0.39901478 0.39408867 0.38916256 0.4679803 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shahi\\anaconda3\\envs\\fyp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "attractive-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.47      0.39        34\n",
      "           1       0.38      0.24      0.29        34\n",
      "           2       0.24      0.29      0.27        34\n",
      "           3       0.37      0.32      0.34        34\n",
      "           4       0.40      0.24      0.30        34\n",
      "           5       0.66      0.85      0.74        34\n",
      "\n",
      "    accuracy                           0.40       204\n",
      "   macro avg       0.40      0.40      0.39       204\n",
      "weighted avg       0.40      0.40      0.39       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38        34\n",
      "           1       0.26      0.26      0.26        34\n",
      "           2       0.28      0.32      0.30        34\n",
      "           3       0.28      0.21      0.24        34\n",
      "           4       0.32      0.26      0.29        34\n",
      "           5       0.76      0.94      0.84        34\n",
      "\n",
      "    accuracy                           0.40       204\n",
      "   macro avg       0.38      0.40      0.39       204\n",
      "weighted avg       0.38      0.40      0.39       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.38      0.39        34\n",
      "           1       0.22      0.21      0.21        34\n",
      "           2       0.27      0.18      0.21        34\n",
      "           3       0.31      0.24      0.27        34\n",
      "           4       0.50      0.68      0.58        34\n",
      "           5       0.71      0.94      0.81        34\n",
      "\n",
      "    accuracy                           0.44       204\n",
      "   macro avg       0.40      0.44      0.41       204\n",
      "weighted avg       0.40      0.44      0.41       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.38      0.34        34\n",
      "           1       0.28      0.21      0.24        34\n",
      "           2       0.38      0.29      0.33        34\n",
      "           3       0.16      0.15      0.15        34\n",
      "           4       0.56      0.53      0.55        34\n",
      "           5       0.72      1.00      0.84        34\n",
      "\n",
      "    accuracy                           0.43       204\n",
      "   macro avg       0.40      0.43      0.41       204\n",
      "weighted avg       0.40      0.43      0.41       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.39      0.40        33\n",
      "           1       0.27      0.29      0.28        34\n",
      "           2       0.35      0.24      0.28        34\n",
      "           3       0.42      0.38      0.40        34\n",
      "           4       0.40      0.41      0.41        34\n",
      "           5       0.76      1.00      0.86        34\n",
      "\n",
      "    accuracy                           0.45       203\n",
      "   macro avg       0.43      0.45      0.44       203\n",
      "weighted avg       0.43      0.45      0.44       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.41      0.35        34\n",
      "           1       0.37      0.29      0.33        34\n",
      "           2       0.27      0.21      0.24        33\n",
      "           3       0.44      0.41      0.42        34\n",
      "           4       0.57      0.38      0.46        34\n",
      "           5       0.65      0.94      0.77        34\n",
      "\n",
      "    accuracy                           0.44       203\n",
      "   macro avg       0.43      0.44      0.43       203\n",
      "weighted avg       0.43      0.44      0.43       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.38      0.38        34\n",
      "           1       0.20      0.15      0.17        33\n",
      "           2       0.42      0.53      0.47        34\n",
      "           3       0.38      0.32      0.35        34\n",
      "           4       0.61      0.59      0.60        34\n",
      "           5       0.89      1.00      0.94        34\n",
      "\n",
      "    accuracy                           0.50       203\n",
      "   macro avg       0.48      0.50      0.48       203\n",
      "weighted avg       0.48      0.50      0.49       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.47      0.46        34\n",
      "           1       0.42      0.38      0.40        34\n",
      "           2       0.41      0.38      0.39        34\n",
      "           3       0.44      0.42      0.43        33\n",
      "           4       0.68      0.56      0.61        34\n",
      "           5       0.76      1.00      0.86        34\n",
      "\n",
      "    accuracy                           0.54       203\n",
      "   macro avg       0.53      0.54      0.53       203\n",
      "weighted avg       0.53      0.54      0.53       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.26      0.30        34\n",
      "           1       0.44      0.32      0.37        34\n",
      "           2       0.37      0.41      0.39        34\n",
      "           3       0.39      0.44      0.42        34\n",
      "           4       0.63      0.52      0.57        33\n",
      "           5       0.69      0.97      0.80        34\n",
      "\n",
      "    accuracy                           0.49       203\n",
      "   macro avg       0.48      0.49      0.47       203\n",
      "weighted avg       0.47      0.49      0.47       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.29      0.34        34\n",
      "           1       0.40      0.29      0.34        34\n",
      "           2       0.41      0.56      0.47        34\n",
      "           3       0.50      0.47      0.48        34\n",
      "           4       0.59      0.68      0.63        34\n",
      "           5       0.81      0.91      0.86        33\n",
      "\n",
      "    accuracy                           0.53       203\n",
      "   macro avg       0.52      0.53      0.52       203\n",
      "weighted avg       0.52      0.53      0.52       203\n",
      "\n",
      "[0.40196078 0.39705882 0.43627451 0.42647059 0.45320197 0.44334975\n",
      " 0.49753695 0.53694581 0.48768473 0.5320197 ]\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "every-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.29      0.36        34\n",
      "           1       0.44      0.35      0.39        34\n",
      "           2       0.38      0.29      0.33        34\n",
      "           3       0.53      0.56      0.54        34\n",
      "           4       0.68      0.88      0.77        34\n",
      "           5       0.69      1.00      0.82        34\n",
      "\n",
      "    accuracy                           0.56       204\n",
      "   macro avg       0.53      0.56      0.54       204\n",
      "weighted avg       0.53      0.56      0.54       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.24      0.24        34\n",
      "           1       0.42      0.32      0.37        34\n",
      "           2       0.38      0.26      0.31        34\n",
      "           3       0.50      0.53      0.51        34\n",
      "           4       0.63      0.76      0.69        34\n",
      "           5       0.76      1.00      0.86        34\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.49      0.52      0.50       204\n",
      "weighted avg       0.49      0.52      0.50       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.26      0.30        34\n",
      "           1       0.40      0.29      0.34        34\n",
      "           2       0.52      0.38      0.44        34\n",
      "           3       0.45      0.41      0.43        34\n",
      "           4       0.58      0.91      0.71        34\n",
      "           5       0.75      0.97      0.85        34\n",
      "\n",
      "    accuracy                           0.54       204\n",
      "   macro avg       0.51      0.54      0.51       204\n",
      "weighted avg       0.51      0.54      0.51       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.09      0.12        34\n",
      "           1       0.32      0.38      0.35        34\n",
      "           2       0.48      0.29      0.36        34\n",
      "           3       0.57      0.59      0.58        34\n",
      "           4       0.72      0.85      0.78        34\n",
      "           5       0.69      1.00      0.82        34\n",
      "\n",
      "    accuracy                           0.53       204\n",
      "   macro avg       0.49      0.53      0.50       204\n",
      "weighted avg       0.49      0.53      0.50       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.18      0.20        33\n",
      "           1       0.27      0.26      0.27        34\n",
      "           2       0.39      0.26      0.32        34\n",
      "           3       0.58      0.74      0.65        34\n",
      "           4       0.66      0.68      0.67        34\n",
      "           5       0.80      0.97      0.88        34\n",
      "\n",
      "    accuracy                           0.52       203\n",
      "   macro avg       0.49      0.52      0.50       203\n",
      "weighted avg       0.49      0.52      0.50       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.15      0.20        34\n",
      "           1       0.40      0.35      0.38        34\n",
      "           2       0.45      0.30      0.36        33\n",
      "           3       0.61      0.68      0.64        34\n",
      "           4       0.71      0.85      0.77        34\n",
      "           5       0.61      1.00      0.76        34\n",
      "\n",
      "    accuracy                           0.56       203\n",
      "   macro avg       0.51      0.56      0.52       203\n",
      "weighted avg       0.51      0.56      0.52       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.29      0.37        34\n",
      "           1       0.48      0.39      0.43        33\n",
      "           2       0.55      0.53      0.54        34\n",
      "           3       0.78      0.82      0.80        34\n",
      "           4       0.73      0.97      0.84        34\n",
      "           5       0.81      1.00      0.89        34\n",
      "\n",
      "    accuracy                           0.67       203\n",
      "   macro avg       0.64      0.67      0.65       203\n",
      "weighted avg       0.64      0.67      0.65       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.24      0.31        34\n",
      "           1       0.53      0.68      0.60        34\n",
      "           2       0.55      0.53      0.54        34\n",
      "           3       0.62      0.70      0.66        33\n",
      "           4       0.78      0.85      0.82        34\n",
      "           5       0.92      0.97      0.94        34\n",
      "\n",
      "    accuracy                           0.66       203\n",
      "   macro avg       0.65      0.66      0.64       203\n",
      "weighted avg       0.65      0.66      0.64       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.32      0.42        34\n",
      "           1       0.59      0.68      0.63        34\n",
      "           2       0.62      0.53      0.57        34\n",
      "           3       0.71      0.79      0.75        34\n",
      "           4       0.81      0.88      0.84        33\n",
      "           5       0.79      1.00      0.88        34\n",
      "\n",
      "    accuracy                           0.70       203\n",
      "   macro avg       0.69      0.70      0.68       203\n",
      "weighted avg       0.69      0.70      0.68       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.29      0.36        34\n",
      "           1       0.73      0.65      0.69        34\n",
      "           2       0.69      0.53      0.60        34\n",
      "           3       0.69      0.85      0.76        34\n",
      "           4       0.77      0.97      0.86        34\n",
      "           5       0.78      0.97      0.86        33\n",
      "\n",
      "    accuracy                           0.71       203\n",
      "   macro avg       0.69      0.71      0.69       203\n",
      "weighted avg       0.69      0.71      0.69       203\n",
      "\n",
      "[0.56372549 0.51960784 0.53921569 0.53431373 0.51724138 0.55665025\n",
      " 0.66995074 0.66009852 0.69950739 0.70935961]\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "outstanding-glory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.32      0.28        34\n",
      "           1       0.47      0.44      0.45        34\n",
      "           2       0.36      0.26      0.31        34\n",
      "           3       0.52      0.50      0.51        34\n",
      "           4       0.62      0.74      0.68        34\n",
      "           5       1.00      0.88      0.94        34\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.54      0.52      0.53       204\n",
      "weighted avg       0.54      0.52      0.53       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.24      0.25        34\n",
      "           1       0.42      0.38      0.40        34\n",
      "           2       0.38      0.38      0.38        34\n",
      "           3       0.50      0.50      0.50        34\n",
      "           4       0.49      0.59      0.53        34\n",
      "           5       0.97      0.97      0.97        34\n",
      "\n",
      "    accuracy                           0.51       204\n",
      "   macro avg       0.50      0.51      0.51       204\n",
      "weighted avg       0.50      0.51      0.51       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.47      0.44        34\n",
      "           1       0.43      0.29      0.35        34\n",
      "           2       0.41      0.32      0.36        34\n",
      "           3       0.45      0.38      0.41        34\n",
      "           4       0.50      0.79      0.61        34\n",
      "           5       0.97      0.94      0.96        34\n",
      "\n",
      "    accuracy                           0.53       204\n",
      "   macro avg       0.53      0.53      0.52       204\n",
      "weighted avg       0.53      0.53      0.52       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.24      0.27        34\n",
      "           1       0.47      0.59      0.52        34\n",
      "           2       0.52      0.41      0.46        34\n",
      "           3       0.55      0.47      0.51        34\n",
      "           4       0.70      0.88      0.78        34\n",
      "           5       0.94      1.00      0.97        34\n",
      "\n",
      "    accuracy                           0.60       204\n",
      "   macro avg       0.58      0.60      0.58       204\n",
      "weighted avg       0.58      0.60      0.58       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.27      0.31        33\n",
      "           1       0.22      0.15      0.18        34\n",
      "           2       0.23      0.21      0.22        34\n",
      "           3       0.60      0.79      0.68        34\n",
      "           4       0.57      0.76      0.65        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.53       203\n",
      "   macro avg       0.50      0.53      0.51       203\n",
      "weighted avg       0.50      0.53      0.51       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.26      0.29        34\n",
      "           1       0.48      0.32      0.39        34\n",
      "           2       0.47      0.48      0.48        33\n",
      "           3       0.55      0.65      0.59        34\n",
      "           4       0.64      0.82      0.72        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.59       203\n",
      "   macro avg       0.58      0.59      0.58       203\n",
      "weighted avg       0.58      0.59      0.58       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.24      0.25        34\n",
      "           1       0.39      0.39      0.39        33\n",
      "           2       0.48      0.44      0.46        34\n",
      "           3       0.53      0.53      0.53        34\n",
      "           4       0.68      0.82      0.75        34\n",
      "           5       1.00      0.97      0.99        34\n",
      "\n",
      "    accuracy                           0.57       203\n",
      "   macro avg       0.56      0.57      0.56       203\n",
      "weighted avg       0.56      0.57      0.56       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.26      0.30        34\n",
      "           1       0.62      0.59      0.61        34\n",
      "           2       0.54      0.62      0.58        34\n",
      "           3       0.70      0.70      0.70        33\n",
      "           4       0.71      0.79      0.75        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.66       203\n",
      "   macro avg       0.65      0.66      0.65       203\n",
      "weighted avg       0.65      0.66      0.65       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.29      0.32        34\n",
      "           1       0.50      0.62      0.55        34\n",
      "           2       0.60      0.53      0.56        34\n",
      "           3       0.71      0.59      0.65        34\n",
      "           4       0.70      0.79      0.74        33\n",
      "           5       0.92      1.00      0.96        34\n",
      "\n",
      "    accuracy                           0.64       203\n",
      "   macro avg       0.63      0.64      0.63       203\n",
      "weighted avg       0.63      0.64      0.63       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.24      0.27        34\n",
      "           1       0.48      0.47      0.48        34\n",
      "           2       0.54      0.56      0.55        34\n",
      "           3       0.42      0.53      0.47        34\n",
      "           4       0.70      0.68      0.69        34\n",
      "           5       0.97      0.97      0.97        33\n",
      "\n",
      "    accuracy                           0.57       203\n",
      "   macro avg       0.57      0.57      0.57       203\n",
      "weighted avg       0.57      0.57      0.57       203\n",
      "\n",
      "[0.5245098  0.50980392 0.53431373 0.59803922 0.5320197  0.591133\n",
      " 0.56650246 0.66009852 0.63546798 0.57142857]\n"
     ]
    }
   ],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "spoken-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.38      0.34        34\n",
      "           1       0.45      0.26      0.33        34\n",
      "           2       0.25      0.24      0.24        34\n",
      "           3       0.33      0.56      0.41        34\n",
      "           4       0.69      0.32      0.44        34\n",
      "           5       0.94      1.00      0.97        34\n",
      "\n",
      "    accuracy                           0.46       204\n",
      "   macro avg       0.49      0.46      0.46       204\n",
      "weighted avg       0.49      0.46      0.46       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.38      0.41        34\n",
      "           1       0.26      0.24      0.25        34\n",
      "           2       0.21      0.18      0.19        34\n",
      "           3       0.22      0.32      0.26        34\n",
      "           4       0.33      0.26      0.30        34\n",
      "           5       0.94      1.00      0.97        34\n",
      "\n",
      "    accuracy                           0.40       204\n",
      "   macro avg       0.40      0.40      0.39       204\n",
      "weighted avg       0.40      0.40      0.39       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.29      0.32        34\n",
      "           1       0.25      0.24      0.24        34\n",
      "           2       0.24      0.21      0.22        34\n",
      "           3       0.30      0.35      0.32        34\n",
      "           4       0.46      0.53      0.49        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.44       204\n",
      "   macro avg       0.43      0.44      0.43       204\n",
      "weighted avg       0.43      0.44      0.43       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31        34\n",
      "           1       0.40      0.18      0.24        34\n",
      "           2       0.16      0.12      0.14        34\n",
      "           3       0.17      0.32      0.22        34\n",
      "           4       0.50      0.24      0.32        34\n",
      "           5       0.87      1.00      0.93        34\n",
      "\n",
      "    accuracy                           0.37       204\n",
      "   macro avg       0.40      0.37      0.36       204\n",
      "weighted avg       0.40      0.37      0.36       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.36      0.34        33\n",
      "           1       0.23      0.21      0.22        34\n",
      "           2       0.43      0.26      0.33        34\n",
      "           3       0.40      0.62      0.48        34\n",
      "           4       0.36      0.26      0.31        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.45       203\n",
      "   macro avg       0.45      0.45      0.44       203\n",
      "weighted avg       0.45      0.45      0.44       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.35      0.35        34\n",
      "           1       0.22      0.12      0.15        34\n",
      "           2       0.27      0.18      0.22        33\n",
      "           3       0.32      0.68      0.43        34\n",
      "           4       0.78      0.41      0.54        34\n",
      "           5       0.89      1.00      0.94        34\n",
      "\n",
      "    accuracy                           0.46       203\n",
      "   macro avg       0.47      0.46      0.44       203\n",
      "weighted avg       0.47      0.46      0.44       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.50      0.42        34\n",
      "           1       0.38      0.15      0.22        33\n",
      "           2       0.41      0.38      0.39        34\n",
      "           3       0.35      0.59      0.44        34\n",
      "           4       0.63      0.35      0.45        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.50       203\n",
      "   macro avg       0.52      0.50      0.48       203\n",
      "weighted avg       0.52      0.50      0.49       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.50      0.44        34\n",
      "           1       0.14      0.06      0.08        34\n",
      "           2       0.25      0.15      0.19        34\n",
      "           3       0.29      0.61      0.39        33\n",
      "           4       0.58      0.32      0.42        34\n",
      "           5       0.92      1.00      0.96        34\n",
      "\n",
      "    accuracy                           0.44       203\n",
      "   macro avg       0.43      0.44      0.41       203\n",
      "weighted avg       0.43      0.44      0.41       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.38      0.33        34\n",
      "           1       0.29      0.06      0.10        34\n",
      "           2       0.25      0.21      0.23        34\n",
      "           3       0.32      0.62      0.42        34\n",
      "           4       0.53      0.27      0.36        33\n",
      "           5       0.87      1.00      0.93        34\n",
      "\n",
      "    accuracy                           0.42       203\n",
      "   macro avg       0.42      0.42      0.39       203\n",
      "weighted avg       0.42      0.42      0.39       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.32      0.31        34\n",
      "           1       0.23      0.09      0.13        34\n",
      "           2       0.40      0.29      0.34        34\n",
      "           3       0.32      0.59      0.42        34\n",
      "           4       0.61      0.56      0.58        34\n",
      "           5       0.92      1.00      0.96        33\n",
      "\n",
      "    accuracy                           0.47       203\n",
      "   macro avg       0.46      0.48      0.46       203\n",
      "weighted avg       0.46      0.47      0.45       203\n",
      "\n",
      "[0.46078431 0.39705882 0.43627451 0.36764706 0.45320197 0.45812808\n",
      " 0.49753695 0.43842365 0.42364532 0.4729064 ]\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "expensive-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      1.00      0.41        34\n",
      "           1       1.00      0.24      0.38        34\n",
      "           2       1.00      0.15      0.26        34\n",
      "           3       1.00      0.41      0.58        34\n",
      "           4       1.00      0.53      0.69        34\n",
      "           5       1.00      0.82      0.90        34\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.88      0.52      0.54       204\n",
      "weighted avg       0.88      0.52      0.54       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      1.00      0.41        34\n",
      "           1       1.00      0.35      0.52        34\n",
      "           2       1.00      0.18      0.30        34\n",
      "           3       1.00      0.29      0.45        34\n",
      "           4       1.00      0.50      0.67        34\n",
      "           5       1.00      0.79      0.89        34\n",
      "\n",
      "    accuracy                           0.52       204\n",
      "   macro avg       0.88      0.52      0.54       204\n",
      "weighted avg       0.88      0.52      0.54       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      1.00      0.39        34\n",
      "           1       1.00      0.15      0.26        34\n",
      "           2       1.00      0.12      0.21        34\n",
      "           3       1.00      0.35      0.52        34\n",
      "           4       1.00      0.53      0.69        34\n",
      "           5       1.00      0.71      0.83        34\n",
      "\n",
      "    accuracy                           0.48       204\n",
      "   macro avg       0.87      0.48      0.48       204\n",
      "weighted avg       0.87      0.48      0.48       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      1.00      0.39        34\n",
      "           1       1.00      0.15      0.26        34\n",
      "           2       1.00      0.12      0.21        34\n",
      "           3       1.00      0.38      0.55        34\n",
      "           4       1.00      0.41      0.58        34\n",
      "           5       1.00      0.85      0.92        34\n",
      "\n",
      "    accuracy                           0.49       204\n",
      "   macro avg       0.87      0.49      0.49       204\n",
      "weighted avg       0.87      0.49      0.49       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      1.00      0.38        33\n",
      "           1       1.00      0.09      0.16        34\n",
      "           2       1.00      0.12      0.21        34\n",
      "           3       1.00      0.56      0.72        34\n",
      "           4       1.00      0.26      0.42        34\n",
      "           5       1.00      0.82      0.90        34\n",
      "\n",
      "    accuracy                           0.47       203\n",
      "   macro avg       0.87      0.48      0.47       203\n",
      "weighted avg       0.88      0.47      0.47       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      1.00      0.42        34\n",
      "           1       1.00      0.18      0.30        34\n",
      "           2       1.00      0.24      0.39        33\n",
      "           3       1.00      0.53      0.69        34\n",
      "           4       1.00      0.44      0.61        34\n",
      "           5       1.00      0.88      0.94        34\n",
      "\n",
      "    accuracy                           0.55       203\n",
      "   macro avg       0.88      0.55      0.56       203\n",
      "weighted avg       0.88      0.55      0.56       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      1.00      0.38        34\n",
      "           1       1.00      0.09      0.17        33\n",
      "           2       1.00      0.32      0.49        34\n",
      "           3       1.00      0.35      0.52        34\n",
      "           4       1.00      0.29      0.45        34\n",
      "           5       1.00      0.68      0.81        34\n",
      "\n",
      "    accuracy                           0.46       203\n",
      "   macro avg       0.87      0.46      0.47       203\n",
      "weighted avg       0.87      0.46      0.47       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      1.00      0.47        34\n",
      "           1       1.00      0.56      0.72        34\n",
      "           2       1.00      0.41      0.58        34\n",
      "           3       1.00      0.45      0.62        33\n",
      "           4       1.00      0.50      0.67        34\n",
      "           5       1.00      0.76      0.87        34\n",
      "\n",
      "    accuracy                           0.62       203\n",
      "   macro avg       0.88      0.61      0.65       203\n",
      "weighted avg       0.88      0.62      0.65       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      1.00      0.47        34\n",
      "           1       1.00      0.47      0.64        34\n",
      "           2       1.00      0.35      0.52        34\n",
      "           3       1.00      0.53      0.69        34\n",
      "           4       1.00      0.52      0.68        33\n",
      "           5       1.00      0.85      0.92        34\n",
      "\n",
      "    accuracy                           0.62       203\n",
      "   macro avg       0.88      0.62      0.65       203\n",
      "weighted avg       0.88      0.62      0.65       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      1.00      0.42        34\n",
      "           1       1.00      0.38      0.55        34\n",
      "           2       1.00      0.26      0.42        34\n",
      "           3       1.00      0.32      0.49        34\n",
      "           4       1.00      0.47      0.64        34\n",
      "           5       1.00      0.82      0.90        33\n",
      "\n",
      "    accuracy                           0.54       203\n",
      "   macro avg       0.88      0.54      0.57       203\n",
      "weighted avg       0.88      0.54      0.57       203\n",
      "\n",
      "[0.5245098  0.51960784 0.4754902  0.48529412 0.4729064  0.54679803\n",
      " 0.45812808 0.61576355 0.62068966 0.54187192]\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "amateur-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.59      0.48        34\n",
      "           1       0.78      0.41      0.54        34\n",
      "           2       0.55      0.47      0.51        34\n",
      "           3       0.71      0.79      0.75        34\n",
      "           4       0.89      0.94      0.91        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.70       204\n",
      "   macro avg       0.72      0.70      0.70       204\n",
      "weighted avg       0.72      0.70      0.70       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.50      0.52        34\n",
      "           1       0.61      0.65      0.63        34\n",
      "           2       0.45      0.41      0.43        34\n",
      "           3       0.69      0.71      0.70        34\n",
      "           4       0.81      0.88      0.85        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.69       204\n",
      "   macro avg       0.68      0.69      0.69       204\n",
      "weighted avg       0.68      0.69      0.69       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.50      0.43        34\n",
      "           1       0.61      0.50      0.55        34\n",
      "           2       0.58      0.44      0.50        34\n",
      "           3       0.68      0.62      0.65        34\n",
      "           4       0.82      0.97      0.89        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.67       204\n",
      "   macro avg       0.68      0.67      0.67       204\n",
      "weighted avg       0.68      0.67      0.67       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.38      0.39        34\n",
      "           1       0.54      0.62      0.58        34\n",
      "           2       0.54      0.44      0.48        34\n",
      "           3       0.69      0.71      0.70        34\n",
      "           4       0.91      0.94      0.93        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.68       204\n",
      "   macro avg       0.68      0.68      0.68       204\n",
      "weighted avg       0.68      0.68      0.68       204\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.42      0.42        33\n",
      "           1       0.55      0.47      0.51        34\n",
      "           2       0.59      0.38      0.46        34\n",
      "           3       0.70      0.97      0.81        34\n",
      "           4       0.82      0.91      0.86        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.69       203\n",
      "   macro avg       0.68      0.69      0.68       203\n",
      "weighted avg       0.68      0.69      0.68       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.53      0.51        34\n",
      "           1       0.73      0.56      0.63        34\n",
      "           2       0.56      0.45      0.50        33\n",
      "           3       0.74      0.91      0.82        34\n",
      "           4       0.86      0.94      0.90        34\n",
      "           5       0.97      1.00      0.99        34\n",
      "\n",
      "    accuracy                           0.73       203\n",
      "   macro avg       0.73      0.73      0.73       203\n",
      "weighted avg       0.73      0.73      0.73       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.62      0.57        34\n",
      "           1       0.69      0.33      0.45        33\n",
      "           2       0.72      0.68      0.70        34\n",
      "           3       0.76      0.91      0.83        34\n",
      "           4       0.82      0.97      0.89        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.75       203\n",
      "   macro avg       0.75      0.75      0.74       203\n",
      "weighted avg       0.75      0.75      0.74       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.29      0.43        34\n",
      "           1       0.83      0.88      0.86        34\n",
      "           2       0.68      0.94      0.79        34\n",
      "           3       0.81      0.91      0.86        33\n",
      "           4       0.92      1.00      0.96        34\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.84       203\n",
      "   macro avg       0.85      0.84      0.82       203\n",
      "weighted avg       0.85      0.84      0.82       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.41      0.55        34\n",
      "           1       0.77      0.88      0.82        34\n",
      "           2       0.79      0.91      0.85        34\n",
      "           3       0.77      0.88      0.82        34\n",
      "           4       0.94      1.00      0.97        33\n",
      "           5       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.85       203\n",
      "   macro avg       0.85      0.85      0.84       203\n",
      "weighted avg       0.85      0.85      0.83       203\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.38      0.51        34\n",
      "           1       0.81      0.85      0.83        34\n",
      "           2       0.80      0.97      0.88        34\n",
      "           3       0.75      0.88      0.81        34\n",
      "           4       0.92      0.97      0.94        34\n",
      "           5       1.00      1.00      1.00        33\n",
      "\n",
      "    accuracy                           0.84       203\n",
      "   macro avg       0.84      0.84      0.83       203\n",
      "weighted avg       0.84      0.84      0.83       203\n",
      "\n",
      "[0.70098039 0.69117647 0.67156863 0.68137255 0.69458128 0.73399015\n",
      " 0.75369458 0.83743842 0.84729064 0.84236453]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "threaded-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculating accuracy metrics for LDA\n",
    "# lda_model.fit(X_train, y_train)\n",
    "# lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "# print('Accuracy Metrics for LDA:\\n')\n",
    "# print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "# print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "# print(classification_report(y_test, lda_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
