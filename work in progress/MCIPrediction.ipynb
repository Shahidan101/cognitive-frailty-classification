{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secure-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from parse import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romantic-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show classification report for Cross Validation\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust          368\n",
      "prefrail_mci    268\n",
      "prefrail        250\n",
      "mci             142\n",
      "frail_mci        86\n",
      "frail             9\n",
      "Name: condition, dtype: int64\n",
      "\n",
      "####################################################################\n",
      "Number of Rows of Dataframe:\n",
      "1123\n",
      "Number of Columns of Dataframe:\n",
      "59\n",
      "\n",
      "####################################################################\n",
      "Threshold for number of NULLs in a column: 0.1095\n",
      "Number of Columns before Parsing for Too Many NULLs in a column:\n",
      "59\n",
      "Number of Columns after Parsing for Too Many NULLs in a column:\n",
      "51\n",
      "\n",
      "Columns Removed:\n",
      "B1_b5\n",
      "B4_a1\n",
      "B4_a3\n",
      "B4_a4\n",
      "B4_a6\n",
      "B4_b1\n",
      "B4_b3\n",
      "B5_a1\n",
      "\n",
      "####################################################################\n",
      "Number of Rows before Parsing NULLs in data:\n",
      "1123\n",
      "Number of Rows after Parsing NULLs in data:\n",
      "1007\n",
      "\n",
      "####################################################################\n",
      "Number of Columns after dropping A1_2, B1_b4, B2_c3, B4_b2 for inconsistent data types:\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# Pre-parse the dataset\n",
    "data = preprocess(\"rawfile_blood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "animal-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################\n",
      "Labels with frequencies:\n",
      "Frail: 7\n",
      "Frail + MCI: 76\n",
      "MCI: 133\n",
      "Prefrail + MCI: 231\n",
      "Prefrail: 221\n",
      "Robust: 339\n"
     ]
    }
   ],
   "source": [
    "# Initialise counters for each condition\n",
    "frail = 0\n",
    "frail_mci = 0\n",
    "mci = 0\n",
    "prefrail_mci = 0\n",
    "prefrail = 0\n",
    "robust = 0\n",
    "\n",
    "# Count rows of data for each condition\n",
    "for i in range(0, len(data)):\n",
    "\tif data.at[i, 'condition'] == 'frail':\n",
    "\t\tfrail += 1\n",
    "\telif data.at[i, 'condition'] == 'frail_mci':\n",
    "\t\tfrail_mci += 1\n",
    "\telif data.at[i, 'condition'] == 'mci':\n",
    "\t\tmci += 1\n",
    "\telif data.at[i, 'condition'] == 'prefrail_mci':\n",
    "\t\tprefrail_mci += 1\n",
    "\telif data.at[i, 'condition'] == 'prefrail':\n",
    "\t\tprefrail += 1\n",
    "\telif data.at[i, 'condition'] == 'robust':\n",
    "\t\trobust += 1\n",
    "        \n",
    "# Display number of rows (frequency) for each condition (label)\n",
    "print(\"\\n####################################################################\")\n",
    "print(\"Labels with frequencies:\")\n",
    "print(\"Frail:\", frail)\n",
    "print(\"Frail + MCI:\", frail_mci)\n",
    "print(\"MCI:\", mci)\n",
    "print(\"Prefrail + MCI:\", prefrail_mci)\n",
    "print(\"Prefrail:\", prefrail)\n",
    "print(\"Robust:\", robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unauthorized-simon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtag</th>\n",
       "      <th>condition</th>\n",
       "      <th>A1_1</th>\n",
       "      <th>A2_1</th>\n",
       "      <th>A3_1</th>\n",
       "      <th>B1_a</th>\n",
       "      <th>B1_a1</th>\n",
       "      <th>B1_a2</th>\n",
       "      <th>B1_a3</th>\n",
       "      <th>B1_a4</th>\n",
       "      <th>...</th>\n",
       "      <th>B2_d6</th>\n",
       "      <th>B2_d7</th>\n",
       "      <th>B2_d8</th>\n",
       "      <th>B2_d9</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4_a2</th>\n",
       "      <th>B4_a5</th>\n",
       "      <th>B5_a2</th>\n",
       "      <th>B5_a3</th>\n",
       "      <th>B6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ME02646</td>\n",
       "      <td>frail</td>\n",
       "      <td>196</td>\n",
       "      <td>24</td>\n",
       "      <td>46.5</td>\n",
       "      <td>121</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.37</td>\n",
       "      <td>95</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ME03109</td>\n",
       "      <td>frail</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>55.6</td>\n",
       "      <td>142</td>\n",
       "      <td>4.82</td>\n",
       "      <td>0.42</td>\n",
       "      <td>87</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.011</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ME06997</td>\n",
       "      <td>frail</td>\n",
       "      <td>441</td>\n",
       "      <td>20</td>\n",
       "      <td>76.8</td>\n",
       "      <td>105</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.41</td>\n",
       "      <td>90</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>1.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.023</td>\n",
       "      <td>2.14</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ME07149</td>\n",
       "      <td>frail</td>\n",
       "      <td>265</td>\n",
       "      <td>16</td>\n",
       "      <td>47.2</td>\n",
       "      <td>122</td>\n",
       "      <td>4.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>86</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.012</td>\n",
       "      <td>1.06</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ME07700</td>\n",
       "      <td>frail</td>\n",
       "      <td>425</td>\n",
       "      <td>14</td>\n",
       "      <td>31.3</td>\n",
       "      <td>124</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.38</td>\n",
       "      <td>85</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mtag condition  A1_1  A2_1  A3_1  B1_a  B1_a1  B1_a2  B1_a3  B1_a4  ...  \\\n",
       "0  ME02646     frail   196    24  46.5   121   3.93   0.37     95     31  ...   \n",
       "1  ME03109     frail   200    23  55.6   142   4.82   0.42     87     30  ...   \n",
       "2  ME06997     frail   441    20  76.8   105   4.54   0.41     90     30  ...   \n",
       "3  ME07149     frail   265    16  47.2   122   4.53   0.39     86     27  ...   \n",
       "4  ME07700     frail   425    14  31.3   124   4.44   0.38     85     28  ...   \n",
       "\n",
       "   B2_d6  B2_d7  B2_d8  B2_d9   B3  B4_a2  B4_a5  B5_a2  B5_a3   B6  \n",
       "0      7     12     13      6  0.2    6.0  1.011   1.14    4.1  5.9  \n",
       "1      7     20     17     26  3.1    5.0  1.011   3.25    4.6  8.5  \n",
       "2      5     16     19     15  1.4    7.0  1.023   2.14    4.0  6.4  \n",
       "3      8     24     19     21  2.1    5.5  1.012   1.06    4.7  6.1  \n",
       "4      6     20     23     23  6.0    5.5  1.013   1.95    3.8  5.8  \n",
       "\n",
       "[5 rows x 47 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "important-phase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mtag', 'condition', 'A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
       "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
       "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
       "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
       "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
       "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smooth-defeat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "robust    567\n",
       "mci       440\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping MCI and Robust\n",
    "\n",
    "# Grouping:\n",
    "# MCI, Prefrail_MCI, Frail_MCI, --> MCI\n",
    "# Prefrail, Frail, Robust--> Robust\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "\tif data.at[i, 'condition'] == 'frail':\n",
    "\t\tdata.at[i, 'condition'] = 'robust'\n",
    "\telif data.at[i, 'condition'] == 'frail_mci':\n",
    "\t\tdata.at[i, 'condition'] = 'mci'\n",
    "\telif data.at[i, 'condition'] == 'mci':\n",
    "\t\tdata.at[i, 'condition'] = 'mci'\n",
    "\telif data.at[i, 'condition'] == 'prefrail_mci':\n",
    "\t\tdata.at[i, 'condition'] = 'mci'\n",
    "\telif data.at[i, 'condition'] == 'prefrail':\n",
    "\t\tdata.at[i, 'condition'] = 'robust'\n",
    "\telif data.at[i, 'condition'] == 'robust':\n",
    "\t\tdata.at[i, 'condition'] = 'robust'\n",
    "        \n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "different-lighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.584\n",
      "Linear Discriminant Analysis: 0.599\n",
      "K-Nearest Neigbors: 0.525\n",
      "Classification and Regression Trees: 0.584\n",
      "Gaussian Naive Bayes: 0.619\n",
      "Support Vector Machines: 0.584\n",
      "Random Forest Classifier: 0.599\n"
     ]
    }
   ],
   "source": [
    "# Run Classification using 80/20 Train-Test Split\n",
    "\n",
    "# Logistic Regression\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "reflected-wrist",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.63 accuracy with a standard deviation of 0.05\n",
      "Linear Discriminant Analysis: 0.64 accuracy with a standard deviation of 0.06\n",
      "K-Nearest Neighbors: 0.55 accuracy with a standard deviation of 0.06\n",
      "Classification and Regression Trees: 0.52 accuracy with a standard deviation of 0.05\n",
      "Gaussian Naive Bayes: 0.62 accuracy with a standard deviation of 0.03\n",
      "Support Vector Machines: 0.57 accuracy with a standard deviation of 0.00\n",
      "Random Forest Classifier: 0.62 accuracy with a standard deviation of 0.05\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Score\n",
    "\n",
    "# Run Classification using 80/20 Train-Test Split\n",
    "\n",
    "# Logistic Regression\n",
    "    \n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=10)\n",
    "print(\"Logistic Regression: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=10)\n",
    "print(\"Linear Discriminant Analysis: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10)\n",
    "print(\"K-Nearest Neighbors: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10)\n",
    "print(\"Classification and Regression Trees: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10)\n",
    "print(\"Gaussian Naive Bayes: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10)\n",
    "print(\"Support Vector Machines: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10)\n",
    "print(\"Random Forest Classifier: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "according-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.79      0.71        57\n",
      "           1       0.61      0.43      0.51        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.63      0.61      0.61       101\n",
      "weighted avg       0.63      0.63      0.62       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66        57\n",
      "           1       0.55      0.50      0.52        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.59      0.59      0.59       101\n",
      "weighted avg       0.60      0.60      0.60       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.67      0.63        57\n",
      "           1       0.49      0.41      0.44        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.54      0.54      0.54       101\n",
      "weighted avg       0.55      0.55      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.86      0.74        57\n",
      "           1       0.68      0.39      0.49        44\n",
      "\n",
      "    accuracy                           0.65       101\n",
      "   macro avg       0.66      0.62      0.61       101\n",
      "weighted avg       0.66      0.65      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.72        57\n",
      "           1       0.62      0.18      0.28        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.60      0.55      0.50       101\n",
      "weighted avg       0.60      0.59      0.53       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70        57\n",
      "           1       0.60      0.48      0.53        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.63      0.62      0.62       101\n",
      "weighted avg       0.63      0.63      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.82      0.73        57\n",
      "           1       0.67      0.45      0.54        44\n",
      "\n",
      "    accuracy                           0.66       101\n",
      "   macro avg       0.66      0.64      0.64       101\n",
      "weighted avg       0.66      0.66      0.65       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.79      0.75        56\n",
      "           1       0.68      0.59      0.63        44\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.70      0.69      0.69       100\n",
      "weighted avg       0.70      0.70      0.70       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.84      0.75        56\n",
      "           1       0.71      0.50      0.59        44\n",
      "\n",
      "    accuracy                           0.69       100\n",
      "   macro avg       0.70      0.67      0.67       100\n",
      "weighted avg       0.69      0.69      0.68       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.77      0.65        56\n",
      "           1       0.46      0.25      0.32        44\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.51      0.51      0.49       100\n",
      "weighted avg       0.52      0.54      0.51       100\n",
      "\n",
      "[0.63366337 0.6039604  0.55445545 0.65346535 0.59405941 0.63366337\n",
      " 0.66336634 0.7        0.69       0.54      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Showing Cross Validation Score for each iteration\n",
    "\n",
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seasonal-synthetic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.74      0.68        57\n",
      "           1       0.57      0.45      0.51        44\n",
      "\n",
      "    accuracy                           0.61       101\n",
      "   macro avg       0.60      0.60      0.59       101\n",
      "weighted avg       0.61      0.61      0.61       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.53      0.56        57\n",
      "           1       0.47      0.55      0.51        44\n",
      "\n",
      "    accuracy                           0.53       101\n",
      "   macro avg       0.54      0.54      0.53       101\n",
      "weighted avg       0.54      0.53      0.54       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63        57\n",
      "           1       0.49      0.39      0.43        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.54      0.54      0.53       101\n",
      "weighted avg       0.55      0.55      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72        57\n",
      "           1       0.64      0.52      0.57        44\n",
      "\n",
      "    accuracy                           0.66       101\n",
      "   macro avg       0.66      0.65      0.65       101\n",
      "weighted avg       0.66      0.66      0.66       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.88      0.74        57\n",
      "           1       0.68      0.34      0.45        44\n",
      "\n",
      "    accuracy                           0.64       101\n",
      "   macro avg       0.66      0.61      0.59       101\n",
      "weighted avg       0.65      0.64      0.61       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.84      0.77        57\n",
      "           1       0.74      0.57      0.64        44\n",
      "\n",
      "    accuracy                           0.72       101\n",
      "   macro avg       0.73      0.71      0.71       101\n",
      "weighted avg       0.72      0.72      0.72       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.75      0.68        57\n",
      "           1       0.56      0.41      0.47        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.59      0.58      0.58       101\n",
      "weighted avg       0.60      0.60      0.59       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77        56\n",
      "           1       0.72      0.64      0.67        44\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.73      0.72      0.72       100\n",
      "weighted avg       0.73      0.73      0.73       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73        56\n",
      "           1       0.66      0.66      0.66        44\n",
      "\n",
      "    accuracy                           0.70       100\n",
      "   macro avg       0.70      0.70      0.70       100\n",
      "weighted avg       0.70      0.70      0.70       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69        56\n",
      "           1       0.59      0.39      0.47        44\n",
      "\n",
      "    accuracy                           0.61       100\n",
      "   macro avg       0.60      0.59      0.58       100\n",
      "weighted avg       0.60      0.61      0.59       100\n",
      "\n",
      "[0.61386139 0.53465347 0.55445545 0.66336634 0.64356436 0.72277228\n",
      " 0.6039604  0.73       0.7        0.61      ]\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "upset-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.58        57\n",
      "           1       0.42      0.36      0.39        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.49      0.49      0.49       101\n",
      "weighted avg       0.50      0.50      0.50       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.60      0.55        57\n",
      "           1       0.34      0.27      0.30        44\n",
      "\n",
      "    accuracy                           0.46       101\n",
      "   macro avg       0.43      0.43      0.43       101\n",
      "weighted avg       0.44      0.46      0.44       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56        57\n",
      "           1       0.40      0.36      0.38        44\n",
      "\n",
      "    accuracy                           0.49       101\n",
      "   macro avg       0.47      0.47      0.47       101\n",
      "weighted avg       0.48      0.49      0.48       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.77      0.67        57\n",
      "           1       0.52      0.32      0.39        44\n",
      "\n",
      "    accuracy                           0.57       101\n",
      "   macro avg       0.56      0.55      0.53       101\n",
      "weighted avg       0.56      0.57      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67        57\n",
      "           1       0.50      0.27      0.35        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.54      0.53      0.51       101\n",
      "weighted avg       0.55      0.56      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.74      0.68        57\n",
      "           1       0.56      0.43      0.49        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.59      0.58      0.58       101\n",
      "weighted avg       0.60      0.60      0.59       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.60      0.58        57\n",
      "           1       0.44      0.41      0.42        44\n",
      "\n",
      "    accuracy                           0.51       101\n",
      "   macro avg       0.50      0.50      0.50       101\n",
      "weighted avg       0.51      0.51      0.51       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.86      0.73        56\n",
      "           1       0.67      0.36      0.47        44\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.65      0.61      0.60       100\n",
      "weighted avg       0.65      0.64      0.61       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60        56\n",
      "           1       0.47      0.43      0.45        44\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.53      0.53      0.53       100\n",
      "weighted avg       0.54      0.54      0.54       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69        56\n",
      "           1       0.59      0.43      0.50        44\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.61      0.60      0.60       100\n",
      "weighted avg       0.62      0.62      0.61       100\n",
      "\n",
      "[0.5049505  0.45544554 0.48514851 0.57425743 0.56435644 0.6039604\n",
      " 0.51485149 0.64       0.54       0.62      ]\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "statistical-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.47      0.53        57\n",
      "           1       0.46      0.59      0.52        44\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.53      0.53      0.52       101\n",
      "weighted avg       0.54      0.52      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.46      0.52        57\n",
      "           1       0.47      0.61      0.53        44\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.54      0.53      0.52       101\n",
      "weighted avg       0.54      0.52      0.52       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.56      0.57        57\n",
      "           1       0.44      0.45      0.45        44\n",
      "\n",
      "    accuracy                           0.51       101\n",
      "   macro avg       0.51      0.51      0.51       101\n",
      "weighted avg       0.52      0.51      0.52       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.56      0.58        57\n",
      "           1       0.47      0.50      0.48        44\n",
      "\n",
      "    accuracy                           0.53       101\n",
      "   macro avg       0.53      0.53      0.53       101\n",
      "weighted avg       0.54      0.53      0.54       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.63      0.58        57\n",
      "           1       0.38      0.30      0.33        44\n",
      "\n",
      "    accuracy                           0.49       101\n",
      "   macro avg       0.46      0.46      0.46       101\n",
      "weighted avg       0.47      0.49      0.47       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59        57\n",
      "           1       0.45      0.43      0.44        44\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.51      0.51      0.51       101\n",
      "weighted avg       0.52      0.52      0.52       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.46      0.50        57\n",
      "           1       0.42      0.50      0.45        44\n",
      "\n",
      "    accuracy                           0.48       101\n",
      "   macro avg       0.48      0.48      0.47       101\n",
      "weighted avg       0.49      0.48      0.48       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.52      0.55        56\n",
      "           1       0.46      0.52      0.49        44\n",
      "\n",
      "    accuracy                           0.52       100\n",
      "   macro avg       0.52      0.52      0.52       100\n",
      "weighted avg       0.53      0.52      0.52       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.61      0.62        56\n",
      "           1       0.52      0.55      0.53        44\n",
      "\n",
      "    accuracy                           0.58       100\n",
      "   macro avg       0.58      0.58      0.58       100\n",
      "weighted avg       0.58      0.58      0.58       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.71      0.70        56\n",
      "           1       0.61      0.57      0.59        44\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.64      0.64      0.64       100\n",
      "weighted avg       0.65      0.65      0.65       100\n",
      "\n",
      "[0.52475248 0.52475248 0.51485149 0.53465347 0.48514851 0.52475248\n",
      " 0.47524752 0.52       0.58       0.65      ]\n"
     ]
    }
   ],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moved-banner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.74      0.69        57\n",
      "           1       0.58      0.48      0.53        44\n",
      "\n",
      "    accuracy                           0.62       101\n",
      "   macro avg       0.61      0.61      0.61       101\n",
      "weighted avg       0.62      0.62      0.62       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.61      0.64        57\n",
      "           1       0.54      0.59      0.57        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.60      0.60      0.60       101\n",
      "weighted avg       0.61      0.60      0.61       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.81      0.72        57\n",
      "           1       0.63      0.43      0.51        44\n",
      "\n",
      "    accuracy                           0.64       101\n",
      "   macro avg       0.64      0.62      0.62       101\n",
      "weighted avg       0.64      0.64      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72        57\n",
      "           1       0.63      0.39      0.48        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.63      0.61      0.60       101\n",
      "weighted avg       0.63      0.63      0.61       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.88      0.70        57\n",
      "           1       0.53      0.18      0.27        44\n",
      "\n",
      "    accuracy                           0.57       101\n",
      "   macro avg       0.56      0.53      0.49       101\n",
      "weighted avg       0.56      0.57      0.51       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.72      0.65        57\n",
      "           1       0.50      0.36      0.42        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.55      0.54      0.54       101\n",
      "weighted avg       0.55      0.56      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.77      0.68        57\n",
      "           1       0.55      0.36      0.44        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.58      0.57      0.56       101\n",
      "weighted avg       0.59      0.59      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71        56\n",
      "           1       0.63      0.55      0.59        44\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.65      0.65      0.65       100\n",
      "weighted avg       0.66      0.66      0.66       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.86      0.74        56\n",
      "           1       0.70      0.43      0.54        44\n",
      "\n",
      "    accuracy                           0.67       100\n",
      "   macro avg       0.68      0.64      0.64       100\n",
      "weighted avg       0.68      0.67      0.65       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.95      0.75        56\n",
      "           1       0.79      0.25      0.38        44\n",
      "\n",
      "    accuracy                           0.64       100\n",
      "   macro avg       0.70      0.60      0.56       100\n",
      "weighted avg       0.69      0.64      0.58       100\n",
      "\n",
      "[0.62376238 0.6039604  0.64356436 0.63366337 0.57425743 0.56435644\n",
      " 0.59405941 0.66       0.67       0.64      ]\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "corresponding-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        57\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.28      0.50      0.36       101\n",
      "weighted avg       0.32      0.56      0.41       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72        56\n",
      "           1       1.00      0.02      0.04        44\n",
      "\n",
      "    accuracy                           0.57       100\n",
      "   macro avg       0.78      0.51      0.38       100\n",
      "weighted avg       0.76      0.57      0.42       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72        56\n",
      "           1       1.00      0.02      0.04        44\n",
      "\n",
      "    accuracy                           0.57       100\n",
      "   macro avg       0.78      0.51      0.38       100\n",
      "weighted avg       0.76      0.57      0.42       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      1.00      0.72        56\n",
      "           1       0.00      0.00      0.00        44\n",
      "\n",
      "    accuracy                           0.56       100\n",
      "   macro avg       0.28      0.50      0.36       100\n",
      "weighted avg       0.31      0.56      0.40       100\n",
      "\n",
      "[0.56435644 0.56435644 0.56435644 0.56435644 0.56435644 0.56435644\n",
      " 0.56435644 0.57       0.57       0.56      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "south-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.77      0.70        57\n",
      "           1       0.59      0.43      0.50        44\n",
      "\n",
      "    accuracy                           0.62       101\n",
      "   macro avg       0.62      0.60      0.60       101\n",
      "weighted avg       0.62      0.62      0.61       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.46      0.50        57\n",
      "           1       0.44      0.55      0.48        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.50      0.50      0.49       101\n",
      "weighted avg       0.51      0.50      0.50       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69        57\n",
      "           1       0.58      0.41      0.48        44\n",
      "\n",
      "    accuracy                           0.61       101\n",
      "   macro avg       0.60      0.59      0.59       101\n",
      "weighted avg       0.61      0.61      0.60       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70        57\n",
      "           1       0.60      0.48      0.53        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.63      0.62      0.62       101\n",
      "weighted avg       0.63      0.63      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.88      0.72        57\n",
      "           1       0.63      0.27      0.38        44\n",
      "\n",
      "    accuracy                           0.61       101\n",
      "   macro avg       0.62      0.57      0.55       101\n",
      "weighted avg       0.62      0.61      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.77      0.73        57\n",
      "           1       0.66      0.57      0.61        44\n",
      "\n",
      "    accuracy                           0.68       101\n",
      "   macro avg       0.68      0.67      0.67       101\n",
      "weighted avg       0.68      0.68      0.68       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69        57\n",
      "           1       0.58      0.41      0.48        44\n",
      "\n",
      "    accuracy                           0.61       101\n",
      "   macro avg       0.60      0.59      0.59       101\n",
      "weighted avg       0.61      0.61      0.60       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72        56\n",
      "           1       0.65      0.55      0.59        44\n",
      "\n",
      "    accuracy                           0.67       100\n",
      "   macro avg       0.67      0.66      0.66       100\n",
      "weighted avg       0.67      0.67      0.67       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.73        56\n",
      "           1       0.66      0.48      0.55        44\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.66      0.64      0.64       100\n",
      "weighted avg       0.66      0.66      0.65       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.77      0.71        56\n",
      "           1       0.63      0.50      0.56        44\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.65      0.63      0.63       100\n",
      "weighted avg       0.65      0.65      0.64       100\n",
      "\n",
      "[0.62376238 0.4950495  0.61386139 0.63366337 0.61386139 0.68316832\n",
      " 0.61386139 0.67       0.66       0.65      ]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "associate-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for Logistic Regression:\n",
      "\n",
      "0.58416 \n",
      "\n",
      "[[91 27]\n",
      " [57 27]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.77      0.68       118\n",
      "           1       0.50      0.32      0.39        84\n",
      "\n",
      "    accuracy                           0.58       202\n",
      "   macro avg       0.56      0.55      0.54       202\n",
      "weighted avg       0.57      0.58      0.56       202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Logistic Regression\n",
    "log_model.fit(X_train, y_train)\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "meaning-treasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for LDA:\n",
      "\n",
      "0.59901 \n",
      "\n",
      "[[92 26]\n",
      " [55 29]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.78      0.69       118\n",
      "           1       0.53      0.35      0.42        84\n",
      "\n",
      "    accuracy                           0.60       202\n",
      "   macro avg       0.58      0.56      0.56       202\n",
      "weighted avg       0.58      0.60      0.58       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for LDA\n",
    "lda_model.fit(X_train, y_train)\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ongoing-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for KNN:\n",
      "\n",
      "0.52475 \n",
      "\n",
      "[[75 43]\n",
      " [53 31]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.64      0.61       118\n",
      "           1       0.42      0.37      0.39        84\n",
      "\n",
      "    accuracy                           0.52       202\n",
      "   macro avg       0.50      0.50      0.50       202\n",
      "weighted avg       0.52      0.52      0.52       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for KNN\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "institutional-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for CART:\n",
      "\n",
      "0.60891 \n",
      "\n",
      "[[80 38]\n",
      " [41 43]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67       118\n",
      "           1       0.53      0.51      0.52        84\n",
      "\n",
      "    accuracy                           0.61       202\n",
      "   macro avg       0.60      0.59      0.60       202\n",
      "weighted avg       0.61      0.61      0.61       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for CART\n",
    "cart_model.fit(X_train, y_train)\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "german-sullivan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for Gaussian Naive Bayes:\n",
      "\n",
      "0.61881 \n",
      "\n",
      "[[104  14]\n",
      " [ 63  21]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.88      0.73       118\n",
      "           1       0.60      0.25      0.35        84\n",
      "\n",
      "    accuracy                           0.62       202\n",
      "   macro avg       0.61      0.57      0.54       202\n",
      "weighted avg       0.61      0.62      0.57       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Gaussian Naive Bayes\n",
    "gnb_model.fit(X_train, y_train)\n",
    "gnb_pred = gnb_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for Gaussian Naive Bayes:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "signed-nature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for LDA:\n",
      "\n",
      "0.58416 \n",
      "\n",
      "[[118   0]\n",
      " [ 84   0]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74       118\n",
      "           1       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.58       202\n",
      "   macro avg       0.29      0.50      0.37       202\n",
      "weighted avg       0.34      0.58      0.43       202\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\shahi\\anaconda3\\envs\\FYP\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for SVM\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "heard-cooperation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for LDA:\n",
      "\n",
      "0.59901 \n",
      "\n",
      "[[94 24]\n",
      " [57 27]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70       118\n",
      "           1       0.53      0.32      0.40        84\n",
      "\n",
      "    accuracy                           0.60       202\n",
      "   macro avg       0.58      0.56      0.55       202\n",
      "weighted avg       0.58      0.60      0.57       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Random Forest Classifier\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "musical-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using Features provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "olympic-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct mapping for Feature Names\n",
    "featureName_mapping = {\n",
    "    \"A1_1\" : \"Vitamin B12 (pmol/L)\",\n",
    "    \"A1_2\" : \"Serum Folate (nmol/L)\",\n",
    "    \"A2_1\" : \"Serum Homocysteine (µmol/L)\",\n",
    "    \"A3_1\" : \"25-hydroxy Vitamin D (nmol/L)\",\n",
    "    \"B1_a\" : \"Haemoglobin (g/L)\",\n",
    "    \"B1_a1\" : \"RBC (/L)\",\n",
    "    \"B1_a2\" : \"PCV (L/L)\",\n",
    "    \"B1_a3\" : \"MCV (fL)\",\n",
    "    \"B1_a4\" : \"MCH (pg)\",\n",
    "    \"B1_a5\" : \"MCHC (g/L)\",\n",
    "    \"B1_a6\" : \"RDW (%)\",\n",
    "    \"B1_b\" : \"White Cell Count (/L)\",\n",
    "    \"B1_b1\" : \"Neutrophils (/L)\",\n",
    "    \"B1_b2\" : \"Lymphocytes (/L)\",\n",
    "    \"B1_b3\" : \"Monocytes (/L)\",\n",
    "    \"B1_b4\" : \"Eosinophils (/L)\",\n",
    "    \"B1_b5\" : \"Basophils (/L)\",\n",
    "    \"B1_c\" : \"Platelets (/L)\",\n",
    "    \"B1_d\" : \"Glucose (mmol/L)\",\n",
    "    \"B2_a1\" : \"Total Cholesterol (mmol/L)\",\n",
    "    \"B2_a2\" : \"Triglyceride (mmol/L)\",\n",
    "    \"B2_a3\" : \"HDL Cholesterol (mmol/L)\",\n",
    "    \"B2_a4\" : \"LDL Cholesterol (mmol/L)\",\n",
    "    \"B2_a5\" : \"Total Cholesterol/HDL Ratio\",\n",
    "    \"B2_b1\" : \"Sodium (mmol/L)\",\n",
    "    \"B2_b2\" : \"Potassium (mmol/L)\",\n",
    "    \"B2_b3\" : \"Chloride (mmol/L)\",\n",
    "    \"B2_c1\" : 'Urea (mmol/L)',\n",
    "    \"B2_c2\" : \"Creatinine (umol/L)\",\n",
    "    \"B2_c3\" : \"eGFR (mL/min/1.73m2)\",\n",
    "    \"B2_c4\" : \"Uric Acid (mmol/L)\",\n",
    "    \"B2_c5\" : \"Calcium (mmol/L)\",\n",
    "    \"B2_c6\" : \"Corrected Calcium (mmol/L)\",\n",
    "    \"B2_c7\" : \"Phosphate (mmol/L)\",\n",
    "    \"B2_d1\" : \"Total Protein (g/L)\",\n",
    "    \"B2_d2\" : \"Albumin (g/L)\",\n",
    "    \"B2_d3\" : \"Globulin (g/L)\",\n",
    "    \"B2_d4\" : \"Albumin/Globulin ratio\",\n",
    "    \"B2_d5\" : \"Alkaline Phosphatase (U/L)\",\n",
    "    \"B2_d6\" : \"Total Bilirubin (µmol/L)\",\n",
    "    \"B2_d7\" : \"GGT\",\n",
    "    \"B2_d8\" : \"AST\",\n",
    "    \"B2_d9\" : \"ALT\",\n",
    "    \"B3\" : \"C-Reactive Protein\",\n",
    "    \"B4_a1\" : \"Protein\",\n",
    "    \"B4_a2\" : \"pH\",\n",
    "    \"B4_a3\" : \"Glucose\",\n",
    "    \"B4_a4\" : \"Ketones\",\n",
    "    \"B4_a5\" : \"S.G.\",\n",
    "    \"B4_a6\" : \"Blood\",\n",
    "    \"B4_b1\" : \"Leucocytes (/L)\",\n",
    "    \"B4_b2\" : \"Erythrocytes (/L)\",\n",
    "    \"B4_b3\" : \"Epithelial Cells\",\n",
    "    \"B5_a1\" : \"Free Thyroxine (FT4) (pmol/L)\",\n",
    "    \"B5_a2\" : \"Thyroid Stimulating Hormone (mIU/L)\",\n",
    "    \"B5_a3\" : \"Free Tri-iodothyronine (FT3) (pmol/L)\",\n",
    "    \"B6\" : \"HbA1c\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "filled-scotland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "['A1_2', 'A2_1', 'B2_c3', 'B2_d2', 'B5_a3', 'B6']\n",
      "\n",
      "Columns missing in parsed dataset:\n",
      "A1_2 --> Serum Folate (nmol/L)\n",
      "B2_c3 --> eGFR (mL/min/1.73m2)\n",
      "\n",
      "Columns existing in parsed dataset:\n",
      "A2_1 --> Serum Homocysteine (µmol/L)\n",
      "B2_d2 --> Albumin (g/L)\n",
      "B5_a3 --> Free Tri-iodothyronine (FT3) (pmol/L)\n",
      "B6 --> HbA1c\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['A1_2', 'A2_1', 'B2_c3', 'B2_d2', 'B5_a3','B6']\n",
    "missing = []\n",
    "exists = []\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(feature_list)\n",
    "print()\n",
    "\n",
    "for items in feature_list:\n",
    "    if items not in data.columns:\n",
    "        missing.append(items)\n",
    "    else:\n",
    "        exists.append(items)\n",
    "\n",
    "data1 = pd.Series(data=missing, name='MissingFeatures')\n",
    "\n",
    "data1 = data1.map(featureName_mapping)\n",
    "\n",
    "data2 = pd.Series(data=exists, name='ExistingFeatures')\n",
    "\n",
    "data2 = data2.map(featureName_mapping)\n",
    "\n",
    "print(\"Columns missing in parsed dataset:\")\n",
    "\n",
    "for i in range(0, len(data1)):\n",
    "    print(missing[i], \"-->\", data1[i])\n",
    "\n",
    "print(\"\\nColumns existing in parsed dataset:\")\n",
    "\n",
    "for i in range(0, len(data2)):\n",
    "    print(exists[i], \"-->\", data2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "retired-stewart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtag</th>\n",
       "      <th>condition</th>\n",
       "      <th>A2_1</th>\n",
       "      <th>B2_d2</th>\n",
       "      <th>B5_a3</th>\n",
       "      <th>B6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ME02646</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ME03109</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>42</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ME06997</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ME07149</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ME07700</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>MV00454</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>42</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>MV00456</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>MV00460</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>MV00502</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>MV00510</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1007 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mtag  condition  A2_1  B2_d2  B5_a3   B6\n",
       "0     ME02646          0    24     42    4.1  5.9\n",
       "1     ME03109          0    23     42    4.6  8.5\n",
       "2     ME06997          0    20     43    4.0  6.4\n",
       "3     ME07149          0    16     42    4.7  6.1\n",
       "4     ME07700          0    14     45    3.8  5.8\n",
       "...       ...        ...   ...    ...    ...  ...\n",
       "1002  MV00454          0    19     42    4.5  6.2\n",
       "1003  MV00456          0    18     39    3.9  5.6\n",
       "1004  MV00460          0    17     41    4.0  5.6\n",
       "1005  MV00502          0    18     40    4.1  6.0\n",
       "1006  MV00510          0    24     43    4.5  6.2\n",
       "\n",
       "[1007 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data[exists]\n",
    "data1 = data1.reset_index(drop=True)\n",
    "\n",
    "data2 = data[['mtag', 'condition']]\n",
    "data2 = data2.reset_index(drop=True)\n",
    "\n",
    "data_final = data2.join(data1)\n",
    "\n",
    "data = data_final\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subjective-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.604\n",
      "Linear Discriminant Analysis: 0.589\n",
      "K-Nearest Neigbors: 0.55\n",
      "Classification and Regression Trees: 0.564\n",
      "Gaussian Naive Bayes: 0.589\n",
      "Support Vector Machines: 0.564\n",
      "Random Forest Classifier: 0.579\n"
     ]
    }
   ],
   "source": [
    "# Run Classification using 80/20 Train-Test Split\n",
    "\n",
    "# Logistic Regression\n",
    "    \n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "restricted-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.60 accuracy with a standard deviation of 0.05\n",
      "Linear Discriminant Analysis: 0.60 accuracy with a standard deviation of 0.04\n",
      "K-Nearest Neighbors: 0.53 accuracy with a standard deviation of 0.07\n",
      "Classification and Regression Trees: 0.53 accuracy with a standard deviation of 0.04\n",
      "Gaussian Naive Bayes: 0.58 accuracy with a standard deviation of 0.04\n",
      "Support Vector Machines: 0.57 accuracy with a standard deviation of 0.05\n",
      "Random Forest Classifier: 0.55 accuracy with a standard deviation of 0.07\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Score\n",
    "\n",
    "# Run Classification using 80/20 Train-Test Split\n",
    "\n",
    "# Logistic Regression\n",
    "    \n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=10)\n",
    "print(\"Logistic Regression: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=10)\n",
    "print(\"Linear Discriminant Analysis: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10)\n",
    "print(\"K-Nearest Neighbors: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10)\n",
    "print(\"Classification and Regression Trees: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10)\n",
    "print(\"Gaussian Naive Bayes: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10)\n",
    "print(\"Support Vector Machines: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "y = data['condition']\n",
    "features = exists\n",
    "\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "# X = StandardScaler().fit_transform(X_old)\n",
    "# X = MinMaxScaler().fit_transform(X_old)\n",
    "\n",
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "    \n",
    "# Define undersample strategy\n",
    "# sampling_strategy = {0: 83, 1: 83, 2: 83}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10)\n",
    "print(\"Random Forest Classifier: %0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "olympic-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.68      0.62        57\n",
      "           1       0.44      0.32      0.37        44\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.50      0.50      0.49       101\n",
      "weighted avg       0.51      0.52      0.51       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.79      0.71        57\n",
      "           1       0.62      0.45      0.53        44\n",
      "\n",
      "    accuracy                           0.64       101\n",
      "   macro avg       0.64      0.62      0.62       101\n",
      "weighted avg       0.64      0.64      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73        57\n",
      "           1       0.68      0.30      0.41        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.65      0.60      0.57       101\n",
      "weighted avg       0.65      0.63      0.59       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.72        57\n",
      "           1       0.62      0.18      0.28        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.60      0.55      0.50       101\n",
      "weighted avg       0.60      0.59      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.86      0.68        57\n",
      "           1       0.43      0.14      0.21        44\n",
      "\n",
      "    accuracy                           0.54       101\n",
      "   macro avg       0.50      0.50      0.44       101\n",
      "weighted avg       0.50      0.54      0.47       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74        57\n",
      "           1       0.70      0.36      0.48        44\n",
      "\n",
      "    accuracy                           0.65       101\n",
      "   macro avg       0.67      0.62      0.61       101\n",
      "weighted avg       0.66      0.65      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.69        57\n",
      "           1       0.56      0.32      0.41        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.58      0.56      0.55       101\n",
      "weighted avg       0.59      0.59      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.88      0.72        56\n",
      "           1       0.65      0.30      0.41        44\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.63      0.59      0.56       100\n",
      "weighted avg       0.63      0.62      0.58       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75        56\n",
      "           1       0.75      0.34      0.47        44\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.69      0.63      0.61       100\n",
      "weighted avg       0.69      0.66      0.63       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.84      0.67        56\n",
      "           1       0.44      0.16      0.23        44\n",
      "\n",
      "    accuracy                           0.54       100\n",
      "   macro avg       0.50      0.50      0.45       100\n",
      "weighted avg       0.51      0.54      0.48       100\n",
      "\n",
      "[0.52475248 0.64356436 0.63366337 0.59405941 0.54455446 0.65346535\n",
      " 0.59405941 0.62       0.66       0.54      ]\n"
     ]
    }
   ],
   "source": [
    "# Showing Cross Validation Score for each iteration\n",
    "\n",
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "labeled-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66        57\n",
      "           1       0.50      0.32      0.39        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.54      0.54      0.53       101\n",
      "weighted avg       0.55      0.56      0.54       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.84      0.74        57\n",
      "           1       0.68      0.43      0.53        44\n",
      "\n",
      "    accuracy                           0.66       101\n",
      "   macro avg       0.67      0.64      0.63       101\n",
      "weighted avg       0.67      0.66      0.65       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.89      0.73        57\n",
      "           1       0.67      0.27      0.39        44\n",
      "\n",
      "    accuracy                           0.62       101\n",
      "   macro avg       0.64      0.58      0.56       101\n",
      "weighted avg       0.64      0.62      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.91      0.71        57\n",
      "           1       0.58      0.16      0.25        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.58      0.54      0.48       101\n",
      "weighted avg       0.58      0.58      0.51       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.86      0.68        57\n",
      "           1       0.43      0.14      0.21        44\n",
      "\n",
      "    accuracy                           0.54       101\n",
      "   macro avg       0.50      0.50      0.44       101\n",
      "weighted avg       0.50      0.54      0.47       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.88      0.73        57\n",
      "           1       0.67      0.32      0.43        44\n",
      "\n",
      "    accuracy                           0.63       101\n",
      "   macro avg       0.65      0.60      0.58       101\n",
      "weighted avg       0.64      0.63      0.60       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69        57\n",
      "           1       0.54      0.30      0.38        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.57      0.55      0.53       101\n",
      "weighted avg       0.57      0.58      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.89      0.72        56\n",
      "           1       0.67      0.27      0.39        44\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.64      0.58      0.56       100\n",
      "weighted avg       0.63      0.62      0.58       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75        56\n",
      "           1       0.75      0.34      0.47        44\n",
      "\n",
      "    accuracy                           0.66       100\n",
      "   macro avg       0.69      0.63      0.61       100\n",
      "weighted avg       0.69      0.66      0.63       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.84      0.67        56\n",
      "           1       0.40      0.14      0.20        44\n",
      "\n",
      "    accuracy                           0.53       100\n",
      "   macro avg       0.48      0.49      0.44       100\n",
      "weighted avg       0.49      0.53      0.46       100\n",
      "\n",
      "[0.56435644 0.66336634 0.62376238 0.58415842 0.54455446 0.63366337\n",
      " 0.58415842 0.62       0.66       0.53      ]\n"
     ]
    }
   ],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "strange-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55        57\n",
      "           1       0.42      0.43      0.43        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.49      0.49      0.49       101\n",
      "weighted avg       0.50      0.50      0.50       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.64        57\n",
      "           1       0.50      0.41      0.45        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.55      0.55      0.54       101\n",
      "weighted avg       0.56      0.56      0.56       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.58      0.56        57\n",
      "           1       0.40      0.36      0.38        44\n",
      "\n",
      "    accuracy                           0.49       101\n",
      "   macro avg       0.47      0.47      0.47       101\n",
      "weighted avg       0.48      0.49      0.48       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.68      0.63        57\n",
      "           1       0.49      0.39      0.43        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.54      0.54      0.53       101\n",
      "weighted avg       0.55      0.55      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.57        57\n",
      "           1       0.41      0.36      0.39        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.48      0.48      0.48       101\n",
      "weighted avg       0.49      0.50      0.49       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.67      0.65        57\n",
      "           1       0.54      0.50      0.52        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.58      0.58      0.58       101\n",
      "weighted avg       0.59      0.59      0.59       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60        57\n",
      "           1       0.48      0.48      0.48        44\n",
      "\n",
      "    accuracy                           0.54       101\n",
      "   macro avg       0.54      0.54      0.54       101\n",
      "weighted avg       0.54      0.54      0.54       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.61        56\n",
      "           1       0.46      0.39      0.42        44\n",
      "\n",
      "    accuracy                           0.53       100\n",
      "   macro avg       0.52      0.51      0.51       100\n",
      "weighted avg       0.52      0.53      0.52       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68        56\n",
      "           1       0.60      0.61      0.61        44\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.65      0.65      0.65       100\n",
      "weighted avg       0.65      0.65      0.65       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.50      0.47        56\n",
      "           1       0.26      0.23      0.24        44\n",
      "\n",
      "    accuracy                           0.38       100\n",
      "   macro avg       0.36      0.36      0.36       100\n",
      "weighted avg       0.37      0.38      0.37       100\n",
      "\n",
      "[0.4950495  0.56435644 0.48514851 0.55445545 0.4950495  0.59405941\n",
      " 0.54455446 0.53       0.65       0.38      ]\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "patent-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.49      0.52        57\n",
      "           1       0.43      0.50      0.46        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.50      0.50      0.49       101\n",
      "weighted avg       0.50      0.50      0.50       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65        57\n",
      "           1       0.55      0.55      0.55        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.60      0.60      0.60       101\n",
      "weighted avg       0.60      0.60      0.60       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57        57\n",
      "           1       0.46      0.48      0.47        44\n",
      "\n",
      "    accuracy                           0.52       101\n",
      "   macro avg       0.52      0.52      0.52       101\n",
      "weighted avg       0.53      0.52      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.61      0.61        57\n",
      "           1       0.50      0.50      0.50        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.56      0.56      0.56       101\n",
      "weighted avg       0.56      0.56      0.56       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.60      0.60        57\n",
      "           1       0.49      0.50      0.49        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.55      0.55      0.55       101\n",
      "weighted avg       0.56      0.55      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.46      0.50        57\n",
      "           1       0.43      0.52      0.47        44\n",
      "\n",
      "    accuracy                           0.49       101\n",
      "   macro avg       0.49      0.49      0.48       101\n",
      "weighted avg       0.50      0.49      0.49       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.63      0.62        57\n",
      "           1       0.49      0.45      0.47        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.54      0.54      0.54       101\n",
      "weighted avg       0.55      0.55      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.62      0.61        56\n",
      "           1       0.50      0.48      0.49        44\n",
      "\n",
      "    accuracy                           0.56       100\n",
      "   macro avg       0.55      0.55      0.55       100\n",
      "weighted avg       0.56      0.56      0.56       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.68      0.67        56\n",
      "           1       0.57      0.55      0.56        44\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.61      0.61      0.61       100\n",
      "weighted avg       0.62      0.62      0.62       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.46      0.49        56\n",
      "           1       0.40      0.45      0.43        44\n",
      "\n",
      "    accuracy                           0.46       100\n",
      "   macro avg       0.46      0.46      0.46       100\n",
      "weighted avg       0.47      0.46      0.46       100\n",
      "\n",
      "[0.4950495  0.6039604  0.52475248 0.56435644 0.55445545 0.48514851\n",
      " 0.55445545 0.56       0.62       0.46      ]\n"
     ]
    }
   ],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "western-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.72      0.63        57\n",
      "           1       0.41      0.25      0.31        44\n",
      "\n",
      "    accuracy                           0.51       101\n",
      "   macro avg       0.48      0.48      0.47       101\n",
      "weighted avg       0.49      0.51      0.49       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.84      0.73        57\n",
      "           1       0.67      0.41      0.51        44\n",
      "\n",
      "    accuracy                           0.65       101\n",
      "   macro avg       0.66      0.63      0.62       101\n",
      "weighted avg       0.66      0.65      0.63       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.82      0.69        57\n",
      "           1       0.52      0.25      0.34        44\n",
      "\n",
      "    accuracy                           0.57       101\n",
      "   macro avg       0.56      0.54      0.51       101\n",
      "weighted avg       0.56      0.57      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.74        57\n",
      "           1       0.71      0.23      0.34        44\n",
      "\n",
      "    accuracy                           0.62       101\n",
      "   macro avg       0.66      0.58      0.54       101\n",
      "weighted avg       0.65      0.62      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.84      0.68        57\n",
      "           1       0.47      0.18      0.26        44\n",
      "\n",
      "    accuracy                           0.55       101\n",
      "   macro avg       0.52      0.51      0.47       101\n",
      "weighted avg       0.53      0.55      0.50       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.81      0.69        57\n",
      "           1       0.54      0.30      0.38        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.57      0.55      0.53       101\n",
      "weighted avg       0.57      0.58      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.81      0.69        57\n",
      "           1       0.56      0.32      0.41        44\n",
      "\n",
      "    accuracy                           0.59       101\n",
      "   macro avg       0.58      0.56      0.55       101\n",
      "weighted avg       0.59      0.59      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.88      0.71        56\n",
      "           1       0.61      0.25      0.35        44\n",
      "\n",
      "    accuracy                           0.60       100\n",
      "   macro avg       0.60      0.56      0.53       100\n",
      "weighted avg       0.60      0.60      0.55       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.88      0.71        56\n",
      "           1       0.61      0.25      0.35        44\n",
      "\n",
      "    accuracy                           0.60       100\n",
      "   macro avg       0.60      0.56      0.53       100\n",
      "weighted avg       0.60      0.60      0.55       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.89      0.69        56\n",
      "           1       0.45      0.11      0.18        44\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.51      0.50      0.44       100\n",
      "weighted avg       0.51      0.55      0.47       100\n",
      "\n",
      "[0.51485149 0.65346535 0.57425743 0.62376238 0.55445545 0.58415842\n",
      " 0.59405941 0.6        0.6        0.55      ]\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "requested-sewing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.61      0.56        57\n",
      "           1       0.33      0.25      0.29        44\n",
      "\n",
      "    accuracy                           0.46       101\n",
      "   macro avg       0.42      0.43      0.42       101\n",
      "weighted avg       0.44      0.46      0.44       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.69        57\n",
      "           1       0.57      0.36      0.44        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.59      0.58      0.57       101\n",
      "weighted avg       0.60      0.60      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67        57\n",
      "           1       0.52      0.34      0.41        44\n",
      "\n",
      "    accuracy                           0.57       101\n",
      "   macro avg       0.56      0.55      0.54       101\n",
      "weighted avg       0.56      0.57      0.56       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.84      0.71        57\n",
      "           1       0.59      0.30      0.39        44\n",
      "\n",
      "    accuracy                           0.60       101\n",
      "   macro avg       0.60      0.57      0.55       101\n",
      "weighted avg       0.60      0.60      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.79      0.67        57\n",
      "           1       0.50      0.27      0.35        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.54      0.53      0.51       101\n",
      "weighted avg       0.55      0.56      0.53       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.74      0.67        57\n",
      "           1       0.53      0.39      0.45        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.57      0.56      0.56       101\n",
      "weighted avg       0.57      0.58      0.57       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.74      0.66        57\n",
      "           1       0.50      0.34      0.41        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.55      0.54      0.53       101\n",
      "weighted avg       0.55      0.56      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.77      0.66        56\n",
      "           1       0.48      0.27      0.35        44\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.53      0.52      0.50       100\n",
      "weighted avg       0.53      0.55      0.52       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.79      0.72        56\n",
      "           1       0.64      0.48      0.55        44\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.65      0.63      0.63       100\n",
      "weighted avg       0.65      0.65      0.64       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.71      0.62        56\n",
      "           1       0.38      0.23      0.29        44\n",
      "\n",
      "    accuracy                           0.50       100\n",
      "   macro avg       0.46      0.47      0.45       100\n",
      "weighted avg       0.47      0.50      0.47       100\n",
      "\n",
      "[0.45544554 0.6039604  0.57425743 0.6039604  0.56435644 0.58415842\n",
      " 0.56435644 0.55       0.65       0.5       ]\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sixth-complaint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.60      0.57        57\n",
      "           1       0.41      0.36      0.39        44\n",
      "\n",
      "    accuracy                           0.50       101\n",
      "   macro avg       0.48      0.48      0.48       101\n",
      "weighted avg       0.49      0.50      0.49       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.67      0.64        57\n",
      "           1       0.53      0.48      0.50        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.57      0.57      0.57       101\n",
      "weighted avg       0.58      0.58      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.63      0.63        57\n",
      "           1       0.52      0.52      0.52        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.58      0.58      0.58       101\n",
      "weighted avg       0.58      0.58      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.72      0.65        57\n",
      "           1       0.50      0.36      0.42        44\n",
      "\n",
      "    accuracy                           0.56       101\n",
      "   macro avg       0.55      0.54      0.54       101\n",
      "weighted avg       0.55      0.56      0.55       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.72      0.66        57\n",
      "           1       0.52      0.39      0.44        44\n",
      "\n",
      "    accuracy                           0.57       101\n",
      "   macro avg       0.56      0.55      0.55       101\n",
      "weighted avg       0.56      0.57      0.56       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.61      0.62        57\n",
      "           1       0.52      0.55      0.53        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.58      0.58      0.58       101\n",
      "weighted avg       0.59      0.58      0.59       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65        57\n",
      "           1       0.53      0.45      0.49        44\n",
      "\n",
      "    accuracy                           0.58       101\n",
      "   macro avg       0.57      0.57      0.57       101\n",
      "weighted avg       0.58      0.58      0.58       101\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58        56\n",
      "           1       0.47      0.45      0.46        44\n",
      "\n",
      "    accuracy                           0.53       100\n",
      "   macro avg       0.52      0.52      0.52       100\n",
      "weighted avg       0.53      0.53      0.53       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66        56\n",
      "           1       0.57      0.57      0.57        44\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.61      0.61      0.61       100\n",
      "weighted avg       0.62      0.62      0.62       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.59      0.54        56\n",
      "           1       0.30      0.23      0.26        44\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.40      0.41      0.40       100\n",
      "weighted avg       0.41      0.43      0.41       100\n",
      "\n",
      "[0.4950495  0.58415842 0.58415842 0.56435644 0.57425743 0.58415842\n",
      " 0.58415842 0.53       0.62       0.43      ]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "assigned-royal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for Logistic Regression:\n",
      "\n",
      "0.60396 \n",
      "\n",
      "[[97 21]\n",
      " [59 25]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.71       118\n",
      "           1       0.54      0.30      0.38        84\n",
      "\n",
      "    accuracy                           0.60       202\n",
      "   macro avg       0.58      0.56      0.55       202\n",
      "weighted avg       0.59      0.60      0.57       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Logistic Regression\n",
    "log_model.fit(X_train, y_train)\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "portable-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for LDA:\n",
      "\n",
      "0.58911 \n",
      "\n",
      "[[99 19]\n",
      " [64 20]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.84      0.70       118\n",
      "           1       0.51      0.24      0.33        84\n",
      "\n",
      "    accuracy                           0.59       202\n",
      "   macro avg       0.56      0.54      0.51       202\n",
      "weighted avg       0.57      0.59      0.55       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for LDA\n",
    "lda_model.fit(X_train, y_train)\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "silent-avenue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for KNN:\n",
      "\n",
      "0.5495 \n",
      "\n",
      "[[74 44]\n",
      " [47 37]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.63      0.62       118\n",
      "           1       0.46      0.44      0.45        84\n",
      "\n",
      "    accuracy                           0.55       202\n",
      "   macro avg       0.53      0.53      0.53       202\n",
      "weighted avg       0.55      0.55      0.55       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for KNN\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "opposite-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for CART:\n",
      "\n",
      "0.4901 \n",
      "\n",
      "[[61 57]\n",
      " [46 38]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.52      0.54       118\n",
      "           1       0.40      0.45      0.42        84\n",
      "\n",
      "    accuracy                           0.49       202\n",
      "   macro avg       0.49      0.48      0.48       202\n",
      "weighted avg       0.50      0.49      0.49       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for CART\n",
    "cart_model.fit(X_train, y_train)\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "terminal-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for Gaussian Naive Bayes:\n",
      "\n",
      "0.58911 \n",
      "\n",
      "[[103  15]\n",
      " [ 68  16]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.87      0.71       118\n",
      "           1       0.52      0.19      0.28        84\n",
      "\n",
      "    accuracy                           0.59       202\n",
      "   macro avg       0.56      0.53      0.50       202\n",
      "weighted avg       0.57      0.59      0.53       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Gaussian Naive Bayes\n",
    "gnb_model.fit(X_train, y_train)\n",
    "gnb_pred = gnb_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for Gaussian Naive Bayes:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "played-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for SVM:\n",
      "\n",
      "0.56436 \n",
      "\n",
      "[[92 26]\n",
      " [62 22]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.78      0.68       118\n",
      "           1       0.46      0.26      0.33        84\n",
      "\n",
      "    accuracy                           0.56       202\n",
      "   macro avg       0.53      0.52      0.50       202\n",
      "weighted avg       0.54      0.56      0.53       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for SVM\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "monetary-alert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Metrics for Random Forest:\n",
      "\n",
      "0.59406 \n",
      "\n",
      "[[84 34]\n",
      " [48 36]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.71      0.67       118\n",
      "           1       0.51      0.43      0.47        84\n",
      "\n",
      "    accuracy                           0.59       202\n",
      "   macro avg       0.58      0.57      0.57       202\n",
      "weighted avg       0.59      0.59      0.59       202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy metrics for Random Forest Classifier\n",
    "rfc_model.fit(X_train, y_train)\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Accuracy Metrics for Random Forest:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
