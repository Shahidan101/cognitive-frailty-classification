{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from collections import Counter\n",
    "from parser_final import RobustFrailMCIpreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show classification report for Cross Validation\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct High Correlation Filter\n",
    "\n",
    "# Conduct mapping for Feature Names\n",
    "featureName_mapping = {\n",
    "    \"A1_1\" : \"Vitamin B12 (pmol/L)\",\n",
    "    \"A1_2\" : \"Serum Folate (nmol/L)\",\n",
    "    \"A2_1\" : \"Serum Homocysteine (µmol/L)\",\n",
    "    \"A3_1\" : \"25-hydroxy Vitamin D (nmol/L)\",\n",
    "    \"B1_a\" : \"Haemoglobin (g/L)\",\n",
    "    \"B1_a1\" : \"RBC (/L)\",\n",
    "    \"B1_a2\" : \"PCV (L/L)\",\n",
    "    \"B1_a3\" : \"MCV (fL)\",\n",
    "    \"B1_a4\" : \"MCH (pg)\",\n",
    "    \"B1_a5\" : \"MCHC (g/L)\",\n",
    "    \"B1_a6\" : \"RDW (%)\",\n",
    "    \"B1_b\" : \"White Cell Count (/L)\",\n",
    "    \"B1_b1\" : \"Neutrophils (/L)\",\n",
    "    \"B1_b2\" : \"Lymphocytes (/L)\",\n",
    "    \"B1_b3\" : \"Monocytes (/L)\",\n",
    "    \"B1_b4\" : \"Eosinophils (/L)\",\n",
    "    \"B1_b5\" : \"Basophils (/L)\",\n",
    "    \"B1_c\" : \"Platelets (/L)\",\n",
    "    \"B1_d\" : \"Glucose (mmol/L)\",\n",
    "    \"B2_a1\" : \"Total Cholesterol (mmol/L)\",\n",
    "    \"B2_a2\" : \"Triglyceride (mmol/L)\",\n",
    "    \"B2_a3\" : \"HDL Cholesterol (mmol/L)\",\n",
    "    \"B2_a4\" : \"LDL Cholesterol (mmol/L)\",\n",
    "    \"B2_a5\" : \"Total Cholesterol/HDL Ratio\",\n",
    "    \"B2_b1\" : \"Sodium (mmol/L)\",\n",
    "    \"B2_b2\" : \"Potassium (mmol/L)\",\n",
    "    \"B2_b3\" : \"Chloride (mmol/L)\",\n",
    "    \"B2_c1\" : 'Urea (mmol/L)',\n",
    "    \"B2_c2\" : \"Creatinine (umol/L)\",\n",
    "    \"B2_c3\" : \"eGFR (mL/min/1.73m2)\",\n",
    "    \"B2_c4\" : \"Uric Acid (mmol/L)\",\n",
    "    \"B2_c5\" : \"Calcium (mmol/L)\",\n",
    "    \"B2_c6\" : \"Corrected Calcium (mmol/L)\",\n",
    "    \"B2_c7\" : \"Phosphate (mmol/L)\",\n",
    "    \"B2_d1\" : \"Total Protein (g/L)\",\n",
    "    \"B2_d2\" : \"Albumin (g/L)\",\n",
    "    \"B2_d3\" : \"Globulin (g/L)\",\n",
    "    \"B2_d4\" : \"Albumin/Globulin ratio\",\n",
    "    \"B2_d5\" : \"Alkaline Phosphatase (U/L)\",\n",
    "    \"B2_d6\" : \"Total Bilirubin (µmol/L)\",\n",
    "    \"B2_d7\" : \"GGT\",\n",
    "    \"B2_d8\" : \"AST\",\n",
    "    \"B2_d9\" : \"ALT\",\n",
    "    \"B3\" : \"C-Reactive Protein\",\n",
    "    \"B4_a1\" : \"Protein\",\n",
    "    \"B4_a2\" : \"pH\",\n",
    "    \"B4_a3\" : \"Glucose\",\n",
    "    \"B4_a4\" : \"Ketones\",\n",
    "    \"B4_a5\" : \"S.G.\",\n",
    "    \"B4_a6\" : \"Blood\",\n",
    "    \"B4_b1\" : \"Leucocytes (/L)\",\n",
    "    \"B4_b2\" : \"Erythrocytes (/L)\",\n",
    "    \"B4_b3\" : \"Epithelial Cells\",\n",
    "    \"B5_a1\" : \"Free Thyroxine (FT4) (pmol/L)\",\n",
    "    \"B5_a2\" : \"Thyroid Stimulating Hormone (mIU/L)\",\n",
    "    \"B5_a3\" : \"Free Tri-iodothyronine (FT3) (pmol/L)\",\n",
    "    \"B6\" : \"HbA1c\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: New Dataset, 6 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-parse the dataset\n",
    "data = RobustFrailMCIpreprocess(\"rawfile_blood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "X = StandardScaler().fit_transform(X_old)\n",
    "X = MinMaxScaler().fit_transform(X_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Logistic Regression\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for LDA\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for kNN\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for CART\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for GNB\n",
    "gnb_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for GNB:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Support Vector Machine\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Random Forest Classifier\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# predict probabilities\n",
    "log_probs = log_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "log_probs = log_probs[:, 1]\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "# predict probabilities\n",
    "lda_probs = lda_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lda_probs = lda_probs[:, 1]\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "# predict probabilities\n",
    "knn_probs = knn_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "# predict probabilities\n",
    "cart_probs = cart_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "# predict probabilities\n",
    "gnb_probs = gnb_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "gnb_probs = gnb_probs[:, 1]\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "# predict probabilities\n",
    "svm_probs = svm_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "# predict probabilities\n",
    "rfc_probs = rfc_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "rfc_probs = rfc_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-thumbnail",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# calculate roc curves\n",
    "log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "log_ix = np.argmax(log_gmeans)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"AUC: \", auc(log_fpr, log_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (log_thresholds[log_ix], log_gmeans[log_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Logistic Regression')\n",
    "plt.scatter(log_fpr[log_ix], log_tpr[log_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# calculate roc curves\n",
    "lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "lda_ix = np.argmax(lda_gmeans)\n",
    "print(\"Linear Discriminant Analysis:\")\n",
    "print(\"AUC: \", auc(lda_fpr, lda_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (lda_thresholds[lda_ix], lda_gmeans[lda_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(lda_fpr, lda_tpr, marker='.', label='Linear Discriminant Analysis')\n",
    "plt.scatter(lda_fpr[lda_ix], lda_tpr[lda_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "# calculate roc curves\n",
    "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "knn_ix = np.argmax(knn_gmeans)\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print(\"AUC: \", auc(knn_fpr, knn_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (knn_thresholds[knn_ix], knn_gmeans[knn_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='K-Nearest Neighbor')\n",
    "plt.scatter(knn_fpr[knn_ix], knn_tpr[knn_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree\n",
    "\n",
    "# calculate roc curves\n",
    "cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "cart_ix = np.argmax(cart_gmeans)\n",
    "print(\"Classification and Regression Tree:\")\n",
    "print(\"AUC: \", auc(cart_fpr, cart_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (cart_thresholds[cart_ix], cart_gmeans[cart_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='Classification and Regression Tree')\n",
    "plt.scatter(cart_fpr[cart_ix], cart_tpr[cart_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# calculate roc curves\n",
    "gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "gnb_ix = np.argmax(gnb_gmeans)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"AUC: \", auc(gnb_fpr, gnb_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (gnb_thresholds[gnb_ix], gnb_gmeans[gnb_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')\n",
    "plt.scatter(gnb_fpr[gnb_ix], gnb_tpr[gnb_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# calculate roc curves\n",
    "svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "svm_ix = np.argmax(svm_gmeans)\n",
    "print(\"Support Vector Machines:\")\n",
    "print(\"AUC: \", auc(svm_fpr, svm_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (svm_thresholds[svm_ix], svm_gmeans[svm_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(svm_fpr, svm_tpr, marker='.', label='Support Vector Machines')\n",
    "plt.scatter(svm_fpr[svm_ix], svm_tpr[svm_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-closing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# calculate roc curves\n",
    "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "rfc_ix = np.argmax(rfc_gmeans)\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"AUC: \", auc(rfc_fpr, rfc_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (rfc_thresholds[rfc_ix], rfc_gmeans[rfc_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')\n",
    "plt.scatter(rfc_fpr[rfc_ix], rfc_tpr[rfc_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Logistic Regression: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=5)\n",
    "print(\"Linear Discriminant Analysis: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "print(\"K-Nearest Neighbors: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5)\n",
    "print(\"Classification and Regression Trees: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5)\n",
    "print(\"Gaussian Naive Bayes: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "print(\"Support Vector Machines: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5)\n",
    "print(\"Random Forest Classifier: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(log_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(lda_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(cart_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(svm_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(rfc_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Logistic Regression\n",
    "    rfe = RFE(log_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LOG'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Linear Discriminant Analysis\n",
    "    rfe = RFE(lda_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LDA'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Classification & Regression Trees\n",
    "    rfe = RFE(cart_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['CART'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Support Vector Machines\n",
    "    rfe = RFE(svm_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['SVM'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Random Forest Classifier\n",
    "    rfe = RFE(rfc_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['RFC'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: New Dataset, Robust and Non-Robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-cradle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pre-parse the dataset\n",
    "data = RobustFrailMCIpreprocess(\"rawfile_blood.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data)):\n",
    "\tif data.at[i, 'condition'] == 'frail':\n",
    "\t\tdata.at[i, 'condition'] = 'non-robust'\n",
    "\telif data.at[i, 'condition'] == 'frail_mci':\n",
    "\t\tdata.at[i, 'condition'] = 'non-robust'\n",
    "\telif data.at[i, 'condition'] == 'mci':\n",
    "\t\tdata.at[i, 'condition'] = 'non-robust'\n",
    "\telif data.at[i, 'condition'] == 'prefrail_mci':\n",
    "\t\tdata.at[i, 'condition'] = 'non-robust'\n",
    "\telif data.at[i, 'condition'] == 'prefrail':\n",
    "\t\tdata.at[i, 'condition'] = 'non-robust'\n",
    "\telif data.at[i, 'condition'] == 'robust':\n",
    "\t\tdata.at[i, 'condition'] = 'robust'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X_old = data[features]\n",
    "\n",
    "X = X_old\n",
    "X = StandardScaler().fit_transform(X_old)\n",
    "X = MinMaxScaler().fit_transform(X_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarise the new class distribution\n",
    "counter = Counter(y)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Logistic Regression\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for LDA\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for kNN\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for CART\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for GNB\n",
    "gnb_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for GNB:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Support Vector Machine\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Random Forest Classifier\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# predict probabilities\n",
    "log_probs = log_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "log_probs = log_probs[:, 1]\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "# predict probabilities\n",
    "lda_probs = lda_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lda_probs = lda_probs[:, 1]\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "# predict probabilities\n",
    "knn_probs = knn_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "# predict probabilities\n",
    "cart_probs = cart_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "# predict probabilities\n",
    "gnb_probs = gnb_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "gnb_probs = gnb_probs[:, 1]\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "# predict probabilities\n",
    "svm_probs = svm_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "# predict probabilities\n",
    "rfc_probs = rfc_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "rfc_probs = rfc_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-bikini",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# calculate roc curves\n",
    "log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "log_ix = np.argmax(log_gmeans)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"AUC: \", auc(log_fpr, log_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (log_thresholds[log_ix], log_gmeans[log_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Logistic Regression')\n",
    "plt.scatter(log_fpr[log_ix], log_tpr[log_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# calculate roc curves\n",
    "lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "lda_ix = np.argmax(lda_gmeans)\n",
    "print(\"Linear Discriminant Analysis:\")\n",
    "print(\"AUC: \", auc(lda_fpr, lda_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (lda_thresholds[lda_ix], lda_gmeans[lda_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(lda_fpr, lda_tpr, marker='.', label='Linear Discriminant Analysis')\n",
    "plt.scatter(lda_fpr[lda_ix], lda_tpr[lda_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "# calculate roc curves\n",
    "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "knn_ix = np.argmax(knn_gmeans)\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print(\"AUC: \", auc(knn_fpr, knn_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (knn_thresholds[knn_ix], knn_gmeans[knn_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='K-Nearest Neighbor')\n",
    "plt.scatter(knn_fpr[knn_ix], knn_tpr[knn_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree\n",
    "\n",
    "# calculate roc curves\n",
    "cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "cart_ix = np.argmax(cart_gmeans)\n",
    "print(\"Classification and Regression Tree:\")\n",
    "print(\"AUC: \", auc(cart_fpr, cart_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (cart_thresholds[cart_ix], cart_gmeans[cart_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='Classification and Regression Tree')\n",
    "plt.scatter(cart_fpr[cart_ix], cart_tpr[cart_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# calculate roc curves\n",
    "gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "gnb_ix = np.argmax(gnb_gmeans)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"AUC: \", auc(gnb_fpr, gnb_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (gnb_thresholds[gnb_ix], gnb_gmeans[gnb_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')\n",
    "plt.scatter(gnb_fpr[gnb_ix], gnb_tpr[gnb_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# calculate roc curves\n",
    "svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "svm_ix = np.argmax(svm_gmeans)\n",
    "print(\"Support Vector Machines:\")\n",
    "print(\"AUC: \", auc(svm_fpr, svm_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (svm_thresholds[svm_ix], svm_gmeans[svm_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(svm_fpr, svm_tpr, marker='.', label='Support Vector Machines')\n",
    "plt.scatter(svm_fpr[svm_ix], svm_tpr[svm_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-massage",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# calculate roc curves\n",
    "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "rfc_ix = np.argmax(rfc_gmeans)\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"AUC: \", auc(rfc_fpr, rfc_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (rfc_thresholds[rfc_ix], rfc_gmeans[rfc_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')\n",
    "plt.scatter(rfc_fpr[rfc_ix], rfc_tpr[rfc_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Logistic Regression: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=5)\n",
    "print(\"Linear Discriminant Analysis: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "print(\"K-Nearest Neighbors: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5)\n",
    "print(\"Classification and Regression Trees: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5)\n",
    "print(\"Gaussian Naive Bayes: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "print(\"Support Vector Machines: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5)\n",
    "print(\"Random Forest Classifier: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(log_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(lda_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(cart_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(svm_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(rfc_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Logistic Regression\n",
    "    rfe = RFE(log_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LOG'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Linear Discriminant Analysis\n",
    "    rfe = RFE(lda_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LDA'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Classification & Regression Trees\n",
    "    rfe = RFE(cart_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['CART'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Support Vector Machines\n",
    "    rfe = RFE(svm_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['SVM'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Random Forest Classifier\n",
    "    rfe = RFE(rfc_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['RFC'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: New Dataset, Robust and Non-Robust (76 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-north",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-parse the dataset\n",
    "data = RobustFrailMCIpreprocess(\"rawfile_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only Frail+MCI and Robust classes\n",
    "\n",
    "df1 = data[data.condition == 'frail_mci']\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "df2 = data[data.condition == 'robust']\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "data = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-speaker",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X = data[features]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-mouth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_strategy = {0: 76, 1: 76}\n",
    "undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "# sampling_strategy = {0: 100, 1: 100}\n",
    "# oversample = SMOTE(sampling_strategy=sampling_strategy)\n",
    "# X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Logistic Regression\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for LDA\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for kNN\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for CART\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for GNB\n",
    "gnb_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for GNB:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Support Vector Machine\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Random Forest Classifier\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# predict probabilities\n",
    "log_probs = log_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "log_probs = log_probs[:, 1]\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "# predict probabilities\n",
    "lda_probs = lda_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lda_probs = lda_probs[:, 1]\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "# predict probabilities\n",
    "knn_probs = knn_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "# predict probabilities\n",
    "cart_probs = cart_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "# predict probabilities\n",
    "gnb_probs = gnb_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "gnb_probs = gnb_probs[:, 1]\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "# predict probabilities\n",
    "svm_probs = svm_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "# predict probabilities\n",
    "rfc_probs = rfc_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "rfc_probs = rfc_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-confirmation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# calculate roc curves\n",
    "log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "log_ix = np.argmax(log_gmeans)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"AUC: \", auc(log_fpr, log_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (log_thresholds[log_ix], log_gmeans[log_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Logistic Regression')\n",
    "plt.scatter(log_fpr[log_ix], log_tpr[log_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# calculate roc curves\n",
    "lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "lda_ix = np.argmax(lda_gmeans)\n",
    "print(\"Linear Discriminant Analysis:\")\n",
    "print(\"AUC: \", auc(lda_fpr, lda_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (lda_thresholds[lda_ix], lda_gmeans[lda_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(lda_fpr, lda_tpr, marker='.', label='Linear Discriminant Analysis')\n",
    "plt.scatter(lda_fpr[lda_ix], lda_tpr[lda_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "# calculate roc curves\n",
    "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "knn_ix = np.argmax(knn_gmeans)\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print(\"AUC: \", auc(knn_fpr, knn_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (knn_thresholds[knn_ix], knn_gmeans[knn_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='K-Nearest Neighbor')\n",
    "plt.scatter(knn_fpr[knn_ix], knn_tpr[knn_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree\n",
    "\n",
    "# calculate roc curves\n",
    "cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "cart_ix = np.argmax(cart_gmeans)\n",
    "print(\"Classification and Regression Tree:\")\n",
    "print(\"AUC: \", auc(cart_fpr, cart_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (cart_thresholds[cart_ix], cart_gmeans[cart_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='Classification and Regression Tree')\n",
    "plt.scatter(cart_fpr[cart_ix], cart_tpr[cart_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# calculate roc curves\n",
    "gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "gnb_ix = np.argmax(gnb_gmeans)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"AUC: \", auc(gnb_fpr, gnb_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (gnb_thresholds[gnb_ix], gnb_gmeans[gnb_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')\n",
    "plt.scatter(gnb_fpr[gnb_ix], gnb_tpr[gnb_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# calculate roc curves\n",
    "svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "svm_ix = np.argmax(svm_gmeans)\n",
    "print(\"Support Vector Machines:\")\n",
    "print(\"AUC: \", auc(svm_fpr, svm_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (svm_thresholds[svm_ix], svm_gmeans[svm_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(svm_fpr, svm_tpr, marker='.', label='Support Vector Machines')\n",
    "plt.scatter(svm_fpr[svm_ix], svm_tpr[svm_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-links",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# calculate roc curves\n",
    "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "rfc_ix = np.argmax(rfc_gmeans)\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"AUC: \", auc(rfc_fpr, rfc_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (rfc_thresholds[rfc_ix], rfc_gmeans[rfc_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')\n",
    "plt.scatter(rfc_fpr[rfc_ix], rfc_tpr[rfc_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Logistic Regression: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=5)\n",
    "print(\"Linear Discriminant Analysis: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "print(\"K-Nearest Neighbors: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5)\n",
    "print(\"Classification and Regression Trees: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5)\n",
    "print(\"Gaussian Naive Bayes: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "print(\"Support Vector Machines: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5)\n",
    "print(\"Random Forest Classifier: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(log_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(lda_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(cart_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(svm_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-annual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(rfc_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Logistic Regression\n",
    "    rfe = RFE(log_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LOG'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Linear Discriminant Analysis\n",
    "    rfe = RFE(lda_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LDA'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Classification & Regression Trees\n",
    "    rfe = RFE(cart_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['CART'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Support Vector Machines\n",
    "    rfe = RFE(svm_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['SVM'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Random Forest Classifier\n",
    "    rfe = RFE(rfc_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['RFC'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 4: New Dataset, Robust and Non-Robust (343 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-legislation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-parse the dataset\n",
    "data = RobustFrailMCIpreprocess(\"rawfile_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only Frail+MCI and Robust classes\n",
    "\n",
    "df1 = data[data.condition == 'frail_mci']\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "df2 = data[data.condition == 'robust']\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "data = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-mother",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X = data[features]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-polish",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampling_strategy = {0: 76, 1: 76}\n",
    "# undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "# X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "# sampling_strategy = {0: 100, 1: 100}\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Logistic Regression\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for LDA\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for kNN\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for CART\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for GNB\n",
    "gnb_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for GNB:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Support Vector Machine\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Random Forest Classifier\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# predict probabilities\n",
    "log_probs = log_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "log_probs = log_probs[:, 1]\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "# predict probabilities\n",
    "lda_probs = lda_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lda_probs = lda_probs[:, 1]\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "# predict probabilities\n",
    "knn_probs = knn_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "# predict probabilities\n",
    "cart_probs = cart_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "# predict probabilities\n",
    "gnb_probs = gnb_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "gnb_probs = gnb_probs[:, 1]\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "# predict probabilities\n",
    "svm_probs = svm_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "# predict probabilities\n",
    "rfc_probs = rfc_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "rfc_probs = rfc_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-uniform",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# calculate roc curves\n",
    "log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "log_ix = np.argmax(log_gmeans)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"AUC: \", auc(log_fpr, log_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (log_thresholds[log_ix], log_gmeans[log_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Logistic Regression')\n",
    "plt.scatter(log_fpr[log_ix], log_tpr[log_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# calculate roc curves\n",
    "lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "lda_ix = np.argmax(lda_gmeans)\n",
    "print(\"Linear Discriminant Analysis:\")\n",
    "print(\"AUC: \", auc(lda_fpr, lda_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (lda_thresholds[lda_ix], lda_gmeans[lda_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(lda_fpr, lda_tpr, marker='.', label='Linear Discriminant Analysis')\n",
    "plt.scatter(lda_fpr[lda_ix], lda_tpr[lda_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "# calculate roc curves\n",
    "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "knn_ix = np.argmax(knn_gmeans)\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print(\"AUC: \", auc(knn_fpr, knn_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (knn_thresholds[knn_ix], knn_gmeans[knn_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='K-Nearest Neighbor')\n",
    "plt.scatter(knn_fpr[knn_ix], knn_tpr[knn_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree\n",
    "\n",
    "# calculate roc curves\n",
    "cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "cart_ix = np.argmax(cart_gmeans)\n",
    "print(\"Classification and Regression Tree:\")\n",
    "print(\"AUC: \", auc(cart_fpr, cart_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (cart_thresholds[cart_ix], cart_gmeans[cart_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='Classification and Regression Tree')\n",
    "plt.scatter(cart_fpr[cart_ix], cart_tpr[cart_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# calculate roc curves\n",
    "gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "gnb_ix = np.argmax(gnb_gmeans)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"AUC: \", auc(gnb_fpr, gnb_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (gnb_thresholds[gnb_ix], gnb_gmeans[gnb_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')\n",
    "plt.scatter(gnb_fpr[gnb_ix], gnb_tpr[gnb_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# calculate roc curves\n",
    "svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "svm_ix = np.argmax(svm_gmeans)\n",
    "print(\"Support Vector Machines:\")\n",
    "print(\"AUC: \", auc(svm_fpr, svm_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (svm_thresholds[svm_ix], svm_gmeans[svm_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(svm_fpr, svm_tpr, marker='.', label='Support Vector Machines')\n",
    "plt.scatter(svm_fpr[svm_ix], svm_tpr[svm_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-samba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# calculate roc curves\n",
    "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "rfc_ix = np.argmax(rfc_gmeans)\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"AUC: \", auc(rfc_fpr, rfc_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (rfc_thresholds[rfc_ix], rfc_gmeans[rfc_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')\n",
    "plt.scatter(rfc_fpr[rfc_ix], rfc_tpr[rfc_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Logistic Regression: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=5)\n",
    "print(\"Linear Discriminant Analysis: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "print(\"K-Nearest Neighbors: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5)\n",
    "print(\"Classification and Regression Trees: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5)\n",
    "print(\"Gaussian Naive Bayes: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "print(\"Support Vector Machines: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5)\n",
    "print(\"Random Forest Classifier: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(log_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(lda_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(cart_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(svm_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(rfc_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Logistic Regression\n",
    "    rfe = RFE(log_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LOG'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Linear Discriminant Analysis\n",
    "    rfe = RFE(lda_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LDA'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Classification & Regression Trees\n",
    "    rfe = RFE(cart_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['CART'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Support Vector Machines\n",
    "    rfe = RFE(svm_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['SVM'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Random Forest Classifier\n",
    "    rfe = RFE(rfc_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['RFC'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 5: New Dataset, Robust and Non-Robust (100 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-canberra",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pre-parse the dataset\n",
    "data = RobustFrailMCIpreprocess(\"rawfile_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only Frail+MCI and Robust classes\n",
    "\n",
    "df1 = data[data.condition == 'frail_mci']\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "df2 = data[data.condition == 'robust']\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "data = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = data['condition'].value_counts()\n",
    "condition = c.index\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(condition)):\n",
    "    data['condition'].replace(condition[i], i, inplace = True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-insulin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['condition']\n",
    "\n",
    "features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "       'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "       'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "       'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "       'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "       'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "X = data[features]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-buyer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_strategy = {0: 76, 1: 76}\n",
    "undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# Transform the dataset using SMOTE\n",
    "sampling_strategy = {0: 100, 1: 100}\n",
    "oversample = SMOTE(sampling_strategy=sampling_strategy)\n",
    "X, y = oversample.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train, y_train)\n",
    "print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X_train, y_train)\n",
    "print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Logistic Regression\n",
    "log_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for Logistic Regression:\\n')\n",
    "print(accuracy_score(y_test, log_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, log_pred), '\\n')\n",
    "print(classification_report(y_test, log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for LDA\n",
    "lda_pred = lda_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for LDA:\\n')\n",
    "print(accuracy_score(y_test, lda_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, lda_pred), '\\n')\n",
    "print(classification_report(y_test, lda_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for kNN\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for KNN:\\n')\n",
    "print(accuracy_score(y_test, knn_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, knn_pred), '\\n')\n",
    "print(classification_report(y_test, knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for CART\n",
    "cart_pred = cart_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for CART:\\n')\n",
    "print(accuracy_score(y_test, cart_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, cart_pred), '\\n')\n",
    "print(classification_report(y_test, cart_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for GNB\n",
    "gnb_pred = log_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for GNB:\\n')\n",
    "print(accuracy_score(y_test, gnb_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, gnb_pred), '\\n')\n",
    "print(classification_report(y_test, gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Support Vector Machine\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for SVM:\\n')\n",
    "print(accuracy_score(y_test, svm_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, svm_pred), '\\n')\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating for Random Forest Classifier\n",
    "rfc_pred = rfc_model.predict(X_test)\n",
    "\n",
    "print('Performance Metrics for RFC:\\n')\n",
    "print(accuracy_score(y_test, rfc_pred).round(5), '\\n')\n",
    "print(confusion_matrix(y_test, rfc_pred), '\\n')\n",
    "print(classification_report(y_test, rfc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# predict probabilities\n",
    "log_probs = log_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "log_probs = log_probs[:, 1]\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "# predict probabilities\n",
    "lda_probs = lda_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lda_probs = lda_probs[:, 1]\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "# predict probabilities\n",
    "knn_probs = knn_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "knn_probs = knn_probs[:, 1]\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "# predict probabilities\n",
    "cart_probs = cart_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "cart_probs = cart_probs[:, 1]\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "# predict probabilities\n",
    "gnb_probs = gnb_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "gnb_probs = gnb_probs[:, 1]\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "# predict probabilities\n",
    "svm_probs = svm_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "svm_probs = svm_probs[:, 1]\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "# predict probabilities\n",
    "rfc_probs = rfc_model.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "rfc_probs = rfc_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-atlanta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# calculate roc curves\n",
    "log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "log_ix = np.argmax(log_gmeans)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"AUC: \", auc(log_fpr, log_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (log_thresholds[log_ix], log_gmeans[log_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(log_fpr, log_tpr, marker='.', label='Logistic Regression')\n",
    "plt.scatter(log_fpr[log_ix], log_tpr[log_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# calculate roc curves\n",
    "lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "lda_ix = np.argmax(lda_gmeans)\n",
    "print(\"Linear Discriminant Analysis:\")\n",
    "print(\"AUC: \", auc(lda_fpr, lda_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (lda_thresholds[lda_ix], lda_gmeans[lda_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(lda_fpr, lda_tpr, marker='.', label='Linear Discriminant Analysis')\n",
    "plt.scatter(lda_fpr[lda_ix], lda_tpr[lda_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbor\n",
    "\n",
    "# calculate roc curves\n",
    "knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "knn_ix = np.argmax(knn_gmeans)\n",
    "print(\"K-Nearest Neighbor:\")\n",
    "print(\"AUC: \", auc(knn_fpr, knn_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (knn_thresholds[knn_ix], knn_gmeans[knn_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(knn_fpr, knn_tpr, marker='.', label='K-Nearest Neighbor')\n",
    "plt.scatter(knn_fpr[knn_ix], knn_tpr[knn_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Tree\n",
    "\n",
    "# calculate roc curves\n",
    "cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "cart_ix = np.argmax(cart_gmeans)\n",
    "print(\"Classification and Regression Tree:\")\n",
    "print(\"AUC: \", auc(cart_fpr, cart_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (cart_thresholds[cart_ix], cart_gmeans[cart_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(cart_fpr, cart_tpr, marker='.', label='Classification and Regression Tree')\n",
    "plt.scatter(cart_fpr[cart_ix], cart_tpr[cart_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "# calculate roc curves\n",
    "gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "gnb_ix = np.argmax(gnb_gmeans)\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"AUC: \", auc(gnb_fpr, gnb_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (gnb_thresholds[gnb_ix], gnb_gmeans[gnb_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(gnb_fpr, gnb_tpr, marker='.', label='Gaussian Naive Bayes')\n",
    "plt.scatter(gnb_fpr[gnb_ix], gnb_tpr[gnb_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# calculate roc curves\n",
    "svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "svm_ix = np.argmax(svm_gmeans)\n",
    "print(\"Support Vector Machines:\")\n",
    "print(\"AUC: \", auc(svm_fpr, svm_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (svm_thresholds[svm_ix], svm_gmeans[svm_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(svm_fpr, svm_tpr, marker='.', label='Support Vector Machines')\n",
    "plt.scatter(svm_fpr[svm_ix], svm_tpr[svm_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-committee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# calculate roc curves\n",
    "rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "# calculate the g-mean for each threshold\n",
    "rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "# locate the index of the largest g-mean\n",
    "rfc_ix = np.argmax(rfc_gmeans)\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(\"AUC: \", auc(rfc_fpr, rfc_tpr))\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (rfc_thresholds[rfc_ix], rfc_gmeans[rfc_ix]))\n",
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(rfc_fpr, rfc_tpr, marker='.', label='Random Forest Classifier')\n",
    "plt.scatter(rfc_fpr[rfc_ix], rfc_tpr[rfc_ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X, y)\n",
    "scores = cross_val_score(log_model, X, y, cv=5)\n",
    "print(\"Logistic Regression: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X, y)\n",
    "scores = cross_val_score(lda_model, X, y, cv=5)\n",
    "print(\"Linear Discriminant Analysis: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# K-Nearest Neigbors\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5)\n",
    "print(\"K-Nearest Neighbors: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Classification and Regression Trees\n",
    "\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5)\n",
    "print(\"Classification and Regression Trees: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5)\n",
    "print(\"Gaussian Naive Bayes: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "print(\"Support Vector Machines: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Random Forest Classifier\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5)\n",
    "print(\"Random Forest Classifier: %0.3f accuracy with a standard deviation of %0.3f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "scores = cross_val_score(log_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "scores = cross_val_score(lda_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neigbors\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X, y)\n",
    "scores = cross_val_score(knn_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(X, y)\n",
    "scores = cross_val_score(cart_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X, y)\n",
    "scores = cross_val_score(gnb_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "svm_model = SVC(kernel='linear', gamma = 'auto')\n",
    "svm_model.fit(X, y)\n",
    "scores = cross_val_score(svm_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_model.fit(X, y)\n",
    "scores = cross_val_score(rfc_model, X, y, cv=5, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(log_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(lda_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(cart_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(svm_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "# Create the Recursive Feature Elimination (RFE) model and select 10 attributes\n",
    "rfe = RFE(rfc_model, 10)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# Summarise the selection of the attributes\n",
    "# pd.DataFrame(rfe.support_,index=X_old.columns,columns=['Rank'])\n",
    "rfe_df = pd.DataFrame(rfe.ranking_,index=X_old.columns,columns=['Rank']).sort_values(by='Rank',ascending=True)\n",
    "rfe_df.index = rfe_df.index.map(featureName_mapping)\n",
    "rfe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    # Logistic Regression\n",
    "    rfe = RFE(log_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LOG'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Linear Discriminant Analysis\n",
    "    rfe = RFE(lda_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['LDA'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Classification & Regression Trees\n",
    "    rfe = RFE(cart_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['CART'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Support Vector Machines\n",
    "    rfe = RFE(svm_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['SVM'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    # Random Forest Classifier\n",
    "    rfe = RFE(rfc_model, 10)\n",
    "    model = DecisionTreeClassifier()\n",
    "    models['RFC'] = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "    return models\n",
    "\n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X_train, y_train)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_num = []\n",
    "# log_acc = []\n",
    "# lda_acc = []\n",
    "# knn_acc = []\n",
    "# cart_acc = []\n",
    "# gnb_acc = []\n",
    "# svm_acc = []\n",
    "# rfc_acc = []\n",
    "\n",
    "# log_auc_list = []\n",
    "# lda_auc_list = []\n",
    "# knn_auc_list = []\n",
    "# cart_auc_list = []\n",
    "# gnb_auc_list = []\n",
    "# svm_auc_list = []\n",
    "# rfc_auc_list = []\n",
    "\n",
    "# for classnumber in range(76, 344):\n",
    "#     y = data['condition']\n",
    "\n",
    "#     features = ['A1_1', 'A1_2', 'A2_1', 'A3_1', 'B1_a', 'B1_a1', 'B1_a2',\n",
    "#            'B1_a3', 'B1_a4', 'B1_a5', 'B1_a6', 'B1_b', 'B1_b1', 'B1_b2', 'B1_b3',\n",
    "#            'B1_c', 'B1_d', 'B2_a1', 'B2_a2', 'B2_a3', 'B2_a4', 'B2_a5', 'B2_b1',\n",
    "#            'B2_b2', 'B2_b3', 'B2_c1', 'B2_c2', 'B2_c4', 'B2_c5', 'B2_c6', 'B2_c7',\n",
    "#            'B2_d1', 'B2_d2', 'B2_d3', 'B2_d4', 'B2_d5', 'B2_d6', 'B2_d7', 'B2_d8',\n",
    "#            'B2_d9', 'B3', 'B4_a2', 'B4_a5', 'B5_a2', 'B5_a3', 'B6']\n",
    "#     X = data[features]\n",
    "\n",
    "#     X = StandardScaler().fit_transform(X)\n",
    "#     X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "#     sampling_strategy = {0: classnumber, 1: 76}\n",
    "#     undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "\n",
    "#     X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "#     # Transform the dataset using SMOTE\n",
    "#     oversample = SMOTE()\n",
    "#     X, y = oversample.fit_resample(X, y)\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 1)\n",
    "\n",
    "#     # Logistic Regression\n",
    "\n",
    "#     log_model = LogisticRegression()\n",
    "#     log_model.fit(X_train, y_train)\n",
    "#     log_pred = log_model.predict(X_test)\n",
    "# #     print(\"Logistic Regression:\", log_model.score(X_test, y_test).round(3))\n",
    "#     log_acc.append(log_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # Linear Discriminant Analysis\n",
    "\n",
    "#     lda_model = LinearDiscriminantAnalysis()\n",
    "#     lda_model.fit(X_train, y_train)\n",
    "#     lda_pred = lda_model.predict(X_test)\n",
    "# #     print(\"Linear Discriminant Analysis:\", lda_model.score(X_test, y_test).round(3))\n",
    "#     lda_acc.append(lda_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # K-Nearest Neigbors\n",
    "\n",
    "#     knn_model = KNeighborsClassifier()\n",
    "#     knn_model.fit(X_train, y_train)\n",
    "#     knn_pred = knn_model.predict(X_test)\n",
    "# #     print(\"K-Nearest Neigbors:\", knn_model.score(X_test, y_test).round(3))\n",
    "#     knn_acc.append(knn_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # Classification and Regression Trees\n",
    "\n",
    "#     cart_model = DecisionTreeClassifier()\n",
    "#     cart_model.fit(X_train, y_train)\n",
    "#     cart_pred = cart_model.predict(X_test)\n",
    "# #     print(\"Classification and Regression Trees:\", cart_model.score(X_test, y_test).round(3))\n",
    "#     cart_acc.append(cart_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # Gaussian Naive Bayes\n",
    "\n",
    "#     gnb_model = GaussianNB()\n",
    "#     gnb_model.fit(X_train, y_train)\n",
    "#     gnb_pred = gnb_model.predict(X_test)\n",
    "# #     print(\"Gaussian Naive Bayes:\", gnb_model.score(X_test, y_test).round(3))\n",
    "#     gnb_acc.append(gnb_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # Support Vector Machines\n",
    "\n",
    "#     svm_model = SVC(kernel='linear', gamma = 'auto', probability=True)\n",
    "#     svm_model.fit(X_train, y_train)\n",
    "#     svm_pred = svm_model.predict(X_test)\n",
    "# #     print(\"Support Vector Machines:\", svm_model.score(X_test, y_test).round(3))\n",
    "#     svm_acc.append(svm_model.score(X_test, y_test).round(3))\n",
    "\n",
    "#     # Random Forest Classifier\n",
    "\n",
    "#     rfc_model = RandomForestClassifier()\n",
    "#     rfc_model.fit(X_train, y_train)\n",
    "#     rfc_pred = rfc_model.predict(X_test)\n",
    "# #     print(\"Random Forest Classifier:\", rfc_model.score(X_test, y_test).round(3))\n",
    "#     rfc_acc.append(rfc_model.score(X_test, y_test).round(3))\n",
    "    \n",
    "#     major_num.append(classnumber)\n",
    "    \n",
    "#     # Logistic Regression\n",
    "\n",
    "#     # predict probabilities\n",
    "#     log_probs = log_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     log_probs = log_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     log_fpr, log_tpr, log_thresholds = roc_curve(y_test, log_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     log_gmeans = np.sqrt(log_tpr * (1-log_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     log_ix = np.argmax(log_gmeans)\n",
    "    \n",
    "#     log_auc_list.append(auc(log_fpr, log_tpr))\n",
    "\n",
    "\n",
    "#     # Linear Discriminant Analysis\n",
    "\n",
    "#     # predict probabilities\n",
    "#     lda_probs = lda_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     lda_probs = lda_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     lda_fpr, lda_tpr, lda_thresholds = roc_curve(y_test, lda_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     lda_gmeans = np.sqrt(lda_tpr * (1-lda_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     lda_ix = np.argmax(lda_gmeans)\n",
    "    \n",
    "#     lda_auc_list.append(auc(lda_fpr, lda_tpr))\n",
    "    \n",
    "#     # K-Nearest Neigbors\n",
    "\n",
    "#     # predict probabilities\n",
    "#     knn_probs = knn_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     knn_probs = knn_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     knn_fpr, knn_tpr, knn_thresholds = roc_curve(y_test, knn_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     knn_gmeans = np.sqrt(knn_tpr * (1-knn_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     knn_ix = np.argmax(knn_gmeans)\n",
    "    \n",
    "#     knn_auc_list.append(auc(knn_fpr, knn_tpr))\n",
    "\n",
    "#     # Classification and Regression Trees\n",
    "\n",
    "#     # predict probabilities\n",
    "#     cart_probs = cart_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     cart_probs = cart_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     cart_fpr, cart_tpr, cart_thresholds = roc_curve(y_test, cart_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     cart_gmeans = np.sqrt(cart_tpr * (1-cart_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     cart_ix = np.argmax(cart_gmeans)\n",
    "    \n",
    "#     cart_auc_list.append(auc(cart_fpr, cart_tpr))\n",
    "\n",
    "#     # Gaussian Naive Bayes\n",
    "\n",
    "#     # predict probabilities\n",
    "#     gnb_probs = gnb_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     gnb_probs = gnb_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     gnb_fpr, gnb_tpr, gnb_thresholds = roc_curve(y_test, gnb_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     gnb_gmeans = np.sqrt(gnb_tpr * (1-gnb_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     gnb_ix = np.argmax(gnb_gmeans)\n",
    "    \n",
    "#     gnb_auc_list.append(auc(gnb_fpr, gnb_tpr))\n",
    "\n",
    "#     # Support Vector Machines\n",
    "\n",
    "#     # predict probabilities\n",
    "#     svm_probs = svm_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     svm_probs = svm_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     svm_fpr, svm_tpr, svm_thresholds = roc_curve(y_test, svm_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     svm_gmeans = np.sqrt(svm_tpr * (1-svm_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     svm_ix = np.argmax(svm_gmeans)\n",
    "    \n",
    "#     svm_auc_list.append(auc(svm_fpr, svm_tpr))\n",
    "\n",
    "#     # Random Forest Classifier\n",
    "\n",
    "#     # predict probabilities\n",
    "#     rfc_probs = rfc_model.predict_proba(X_test)\n",
    "#     # keep probabilities for the positive outcome only\n",
    "#     rfc_probs = rfc_probs[:, 1]\n",
    "#     # calculate roc curves\n",
    "#     rfc_fpr, rfc_tpr, rfc_thresholds = roc_curve(y_test, rfc_probs)\n",
    "#     # calculate the g-mean for each threshold\n",
    "#     rfc_gmeans = np.sqrt(rfc_tpr * (1-rfc_fpr))\n",
    "#     # locate the index of the largest g-mean\n",
    "#     rfc_ix = np.argmax(rfc_gmeans)\n",
    "    \n",
    "#     rfc_auc_list.append(auc(rfc_fpr, rfc_tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-browser",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Logistic Regression\n",
    "\n",
    "# plt.plot(major_num, log_acc, label='Logistic Regression')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Discriminant Analysis\n",
    "\n",
    "# plt.plot(major_num, lda_acc, label='Linear Discriminant Analysis')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-Nearest Neighbor\n",
    "\n",
    "# plt.plot(major_num, knn_acc, label='K-Nearest Neighbor')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classification & Regression Tree\n",
    "\n",
    "# plt.plot(major_num, cart_acc, label='Classification & Regression Tree')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gaussian Naive Bayes\n",
    "\n",
    "# plt.plot(major_num, gnb_acc, label='Gaussian Naive Bayes')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Support Vector Machines\n",
    "\n",
    "# plt.plot(major_num, svm_acc, label='Support Vector Machines')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-relaxation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Random Forest Classifier\n",
    "\n",
    "# plt.plot(major_num, rfc_acc, label='Random Forest Classifier')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistic Regression\n",
    "\n",
    "# plt.plot(major_num, log_auc_list, label='Logistic Regression')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Linear Discriminant Analysis\n",
    "\n",
    "# plt.plot(major_num, lda_auc_list, label='Linear Discriminant Analysis')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K-Nearest Neighbor\n",
    "\n",
    "# plt.plot(major_num, knn_auc_list, label='K-Nearest Neighbor')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Classification & Regression Tree\n",
    "\n",
    "# plt.plot(major_num, cart_auc_list, label='Classification & Regression Tree')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gaussian Naive Bayes\n",
    "\n",
    "# plt.plot(major_num, gnb_auc_list, label='Gaussian Naive Bayes')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Support Vector Machines\n",
    "\n",
    "# plt.plot(major_num, svm_auc_list, label='Support Vector Machines')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Classifier\n",
    "\n",
    "# plt.plot(major_num, rfc_auc_list, label='Random Forest Classifier')\n",
    "# # axis labels\n",
    "# plt.xlabel('Majority Class Size')\n",
    "# plt.ylabel('AUC')\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
